title,label,summary
Can Language Models Be Specific? How?,3,A good speaker not only needs to be correct but also has the ability to be specific when desired and so are language models. In this paper we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance given ``J. K. Rowling was born in [MASK].'' we want to test whether a more specific answer will be better filled in by PLMs e.g. Yate instead of England. From our evaluations we show
Blockchain-Based Decentralized Knowledge Marketplace Using Active Inference,2,A knowledge market can be described as a type of market where there is a consistent supply of data to satisfy the demand for information and is responsible for the mapping of potential problem solvers with the entities which need these solutions. It is possible to define them as value-exchange systems in which the dynamic features of the creation and exchange of intellectual assets serve as the fundamental drivers of the frequency nature and outcomes of interactions among various stakeholders. Furthermore the provision of financial backing for research is an essential component in the process of developing a knowledge market
Data Leaves: Scenario-oriented Metadata for Data Federative Innovation,1,A method for representing the digest information of each dataset is proposed oriented to the aid of innovative thoughts and the communication of data users who attempt to create valuable products services and business models using or combining datasets. Compared with methods for connecting datasets via shared attributes (i.e. variables) this method connects datasets via events situations or actions in a scenario that is supposed to be active in the real world. This method reflects the consideration of the fitness of each metadata to the feature concept which is an abstract of the information or knowledge expected to be acquired
iCTGAN--An Attack Mitigation Technique for Random-vector Attack on Accelerometer-based Gait Authentication Systems,2,A recent study showed that commonly (vanilla) studied implementations of accelerometer-based gait authentication systems ($v$ABGait) are susceptible to random-vector attack. The same study proposed a beta noise-assisted implementation ($\beta$ABGait) to mitigate the attack. In this paper we assess the effectiveness of the random-vector attack on both $v$ABGait and $\beta$ABGait using three accelerometer-based gait datasets. In addition we propose $i$ABGait an alternative implementation of ABGait which uses a Conditional Tabular Generative Adversarial Network. Then we evaluate $i$ABGait's resilience against the traditional zero-effort and random-vector attacks. The results show that $i$ABGait mitigates the impact of the random-vector attack to a reasonable extent
Causal Inference for Chatting Handoff,0,Aiming to ensure chatbot quality by predicting chatbot failure and enabling human-agent collaboration Machine-Human Chatting Handoff (MHCH) has attracted lots of attention from both industry and academia in recent years. However most existing methods mainly focus on the dialogue context or assist with global satisfaction prediction based on multi-task learning which ignore the grounded relationships among the causal variables like the user state and labor cost. These variables are significantly associated with handoff decisions resulting in prediction bias and cost increasement. Therefore we propose Causal-Enhance Module (CEM) by establishing the causal graph of MHCH based on these two variables which
Aggregating Crowdsourced and Automatic Judgments to Scale Up a Corpus of Anaphoric Reference for Fiction and Wikipedia Texts,3,Although several datasets annotated for anaphoric reference/coreference exist even the largest such datasets have limitations in terms of size range of domains coverage of anaphoric phenomena and size of documents included. Yet the approaches proposed to scale up anaphoric annotation haven't so far resulted in datasets overcoming these limitations. In this paper we introduce a new release of a corpus for anaphoric reference labelled via a game-with-a-purpose. This new release is comparable in size to the largest existing corpora for anaphoric reference due in part to substantial activity by the players in part thanks to the use of a new
Artificial virtuous agents in a multiagent tragedy of the commons,0,Although virtue ethics has repeatedly been proposed as a suitable framework for the development of artificial moral agents (AMAs) it has been proven difficult to approach from a computational perspective. In this work we present the first technical implementation of artificial virtuous agents (AVAs) in moral simulations. First we review previous conceptual and technical work in artificial virtue ethics and describe a functionalistic path to AVAs based on dispositional virtues bottom-up learning and top-down eudaimonic reward. We then provide the details of a technical implementation in a moral simulation based on a tragedy of the commons scenario. The experimental results
Uncertain Case Identifiers in Process Mining: A User Study of the Event-Case Correlation Problem on Click Data,1,Among the many sources of event data available today a prominent one is user interaction data. User activity may be recorded during the use of an application or website resulting in a type of user interaction data often called click data. An obstacle to the analysis of click data using process mining is the lack of a case identifier in the data. In this paper we show a case and user study for event-case correlation on click data in the context of user interaction events from a mobility sharing company. To reconstruct the case notion of the process we apply
Does Wikidata Support Analogical Reasoning?,0,Analogical reasoning methods have been built over various resources including commonsense knowledge bases lexical resources language models or their combination. While the wide coverage of knowledge about entities and events make Wikidata a promising resource for analogical reasoning across situations and domains Wikidata has not been employed for this task yet. In this paper we investigate whether the knowledge in Wikidata supports analogical reasoning. Specifically we study whether relational knowledge is modeled consistently in Wikidata observing that relevant relational information is typically missing or modeled in an inconsistent way. Our further experiments show that Wikidata can be used to create
Treewidth-aware Reductions of Normal ASP to SAT -- Is Normal ASP Harder than SAT after All?,0,Answer Set Programming (ASP) is a paradigm for modeling and solving problems for knowledge representation and reasoning. There are plenty of results dedicated to studying the hardness of (fragments of) ASP. So far these studies resulted in characterizations in terms of computational complexity as well as in fine-grained insights presented in form of dichotomy-style results lower bounds when translating to other formalisms like propositional satisfiability (SAT) and even detailed parameterized complexity landscapes. A generic parameter in parameterized complexity originating from graph theory is the so-called treewidth which in a sense captures structural density of a program. Recently there was an
Population-Based Reinforcement Learning for Combinatorial Optimization,0,Applying reinforcement learning (RL) to combinatorial optimization problems is attractive as it removes the need for expert knowledge or pre-solved instances. However it is unrealistic to expect an agent to solve these (often NP-)hard problems in a single shot at inference due to their inherent complexity. Thus leading approaches often implement additional search strategies from stochastic sampling and beam-search to explicit fine-tuning. In this paper we argue for the benefits of learning a population of complementary policies which can be simultaneously rolled out at inference. To this end we introduce Poppy a simple theoretically grounded training procedure for populations. Instead
On Explainability in AI-Solutions: A Cross-Domain Survey,0,Artificial Intelligence (AI) increasingly shows its potential to outperform predicate logic algorithms and human control alike. In automatically deriving a system model AI algorithms learn relations in data that are not detectable for humans. This great strength however also makes use of AI methods dubious. The more complex a model the more difficult it is for a human to understand the reasoning for the decisions. As currently fully automated AI algorithms are sparse every algorithm has to provide a reasoning for human operators. For data engineers metrics such as accuracy and sensitivity are sufficient. However if models are interacting with
Fault Diagnosis using eXplainable AI: a Transfer Learning-based Approach for Rotating Machinery exploiting Augmented Synthetic Data,0,Artificial Intelligence (AI) is one of the approaches that has been proposed to analyze the collected data (e.g. vibration signals) providing a diagnosis of the asset's operating condition. It is known that models trained with labeled data (supervised) achieve excellent results but two main problems make their application in production processes difficult: (i) impossibility or long time to obtain a sample of all operational conditions (since faults seldom happen) and (ii) high cost of experts to label all acquired data. Another limitating factor for the applicability of AI approaches in this context is the lack of interpretability of the models
Semantics-based Privacy by Design for Internet of Things Applications,2,As Internet of Things (IoT) technologies become more widespread in everyday life privacy issues are becoming more prominent. The aim of this research is to develop a personal assistant that can answer software engineers' questions about Privacy by Design (PbD) practices during the design phase of IoT system development. Semantic web technologies are used to model the knowledge underlying PbD measurements their intersections with privacy patterns IoT system requirements and the privacy patterns that should be applied across IoT systems. This is achieved through the development of the PARROT ontology developed through a set of representative IoT use cases relevant
A Causal Analysis of Harm,0,"As autonomous systems rapidly become ubiquitous there is a growing need for a legal and regulatory framework to address when and how such a system harms someone. There have been several attempts within the philosophy literature to define harm but none of them has proven capable of dealing with with the many examples that have been presented leading some to suggest that the notion of harm should be abandoned and ""replaced by more well-behaved notions"". As harm is generally something that is caused most of these definitions have involved causality at some level. Yet surprisingly none of them makes use"
Planning Assembly Sequence with Graph Transformer,0,Assembly sequence planning (ASP) is the essential process for modern manufacturing proven to be NP-complete thus its effective and efficient solution has been a challenge for researchers in the field. In this paper we present a graph-transformer based framework for the ASP problem which is trained and demonstrated on a self-collected ASP database. The ASP database contains a self-collected set of LEGO models. The LEGO model is abstracted to a heterogeneous graph structure after a thorough analysis of the original structure and feature extraction. The ground truth assembly sequence is first generated by brute-force search and then adjusted manually to
Understanding the Failure of Batch Normalization for Transformers in NLP,3,Batch Normalization (BN) is a core and prevalent technique in accelerating the training of deep neural networks and improving the generalization on Computer Vision (CV) tasks. However it fails to defend its position in Natural Language Processing (NLP) which is dominated by Layer Normalization (LN). In this paper we are trying to answer why BN usually performs worse than LN in NLP tasks with Transformer models. We find that the inconsistency between training and inference of BN is the leading cause that results in the failure of BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively measure this
Learned k-NN Distance Estimation,1,Big data mining is well known to be an important task for data science because it can provide useful observations and new knowledge hidden in given large datasets. Proximity-based data analysis is particularly utilized in many real-life applications. In such analysis the distances to k nearest neighbors are usually employed thus its main bottleneck is derived from data retrieval. Much efforts have been made to improve the efficiency of these analyses. However they still incur large costs because they essentially need many data accesses. To avoid this issue we propose a machine-learning technique that quickly and accurately estimates the k-NN
Fine-Grained Modeling and Optimization for Intelligent Resource Management in Big Data Processing,1,Big data processing at the production scale presents a highly complex environment for resource optimization (RO) a problem crucial for meeting performance goals and budgetary constraints of analytical users. The RO problem is challenging because it involves a set of decisions (the partition count placement of parallel instances on machines and resource allocation to each instance) requires multi-objective optimization (MOO) and is compounded by the scale and complexity of big data systems while having to meet stringent time constraints for scheduling. This paper presents a MaxCompute-based integrated system to support multi-objective resource optimization via fine-grained instance-level modeling and optimization. We
COVID-19-related Nepali Tweets Classification in a Low Resource Setting,3,Billions of people across the globe have been using social media platforms in their local languages to voice their opinions about the various topics related to the COVID-19 pandemic. Several organizations including the World Health Organization have developed automated social media analysis tools that classify COVID-19-related tweets into various topics. However these tools that help combat the pandemic are limited to very few languages making several countries unable to take their benefit. While multi-lingual or low-resource language-specific tools are being developed they still need to expand their coverage such as for the Nepali language. In this paper we identify the
Enriching Biomedical Knowledge for Low-resource Language Through Translation,3,Biomedical data and benchmarks are highly valuable yet very limited in low-resource languages other than English such as Vietnamese. In this paper we make use of a state-of-the-art translation model in English-Vietnamese to translate and produce both pretrained as well as supervised data in the biomedical domains. Thanks to such large-scale translation we introduce ViPubmedT5 a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus. ViPubMedT5 demonstrates state-of-the-art results on two different biomedical benchmarks in summarization and acronym disambiguation. Further we release ViMedNLI - a new NLP task in Vietnamese translated from MedNLI
Building an Ethereum-Based Decentralized Vehicle Rental System,2,Blockchain technology beyond cryptocurrencies is called to be the new information exchange ecosystem due to its unique properties such as immutability and transparency. The main objective of this work is to introduce the design of a decentralized rental system which leverages smart contracts and the Ethereum public blockchain. The work started from an exhaustive investigation on the Ethereum platform emphasizing the aspect of cryptography and all the technology behind this platform. In order to test the proposed scheme in a realistic use the implementation of a web application for the rental of vehicles has been carried out. The application covers
zkBridge: Trustless Cross-chain Bridges Made Practical,2,Blockchains have seen growing traction with cryptocurrencies reaching a market cap of over 1 trillion dollars major institution investors taking interests and global impacts on governments businesses and individuals. Also growing significantly is the heterogeneity of the ecosystem where a variety of blockchains co-exist. Cross-chain bridge is a necessary building block in this multi-chain ecosystem. Existing solutions however either suffer from performance issues or rely on trust assumptions of committees that significantly lower the security. Recurring attacks against bridges have cost users more than 1.5 billion USD. In this paper we introduce zkBridge an efficient cross-chain bridge that guarantees strong
Checks and Strategies for Enabling Code-Switched Machine Translation,3,Code-switching is a common phenomenon among multilingual speakers where alternation between two or more languages occurs within the context of a single conversation. While multilingual humans can seamlessly switch back and forth between languages multilingual neural machine translation (NMT) models are not robust to such sudden changes in input. This work explores multilingual NMT models' ability to handle code-switched text. First we propose checks to measure switching capability. Second we investigate simple and effective data augmentation methods that can enhance an NMT model's ability to support code-switching. Finally by using a glass-box analysis of attention modules we demonstrate the effectiveness
See Plan Predict: Language-guided Cognitive Planning with Video Prediction,0,Cognitive planning is the structural decomposition of complex tasks into a sequence of future behaviors. In the computational setting performing cognitive planning entails grounding plans and concepts in one or more modalities in order to leverage them for low level control. Since real-world tasks are often described in natural language we devise a cognitive planning algorithm via language-guided video prediction. Current video prediction models do not support conditioning on natural language instructions. Therefore we propose a new video prediction architecture which leverages the power of pre-trained transformers.The network is endowed with the ability to ground concepts based on natural language
Ontologizing Health Systems Data at Scale: Making Translational Discovery a Reality,1,Common data models solve many challenges of standardizing electronic health record (EHR) data but are unable to semantically integrate the resources needed for deep phenotyping. Open Biological and Biomedical Ontology (OBO) Foundry ontologies provide semantically computable representations of biological knowledge and enable the integration of a variety of biomedical data. However mapping EHR data to OBO Foundry ontologies requires significant manual curation and domain expertise. We introduce a framework for mapping Observational Medical Outcomes Partnership (OMOP) standard vocabularies to OBO Foundry ontologies. Using this framework we produced mappings for 92367 conditions 8615 drug ingredients and 10673 measurement results. Mapping accuracy
Duplicate Detection as a Service,1,Completeness of a knowledge graph is an important quality dimension and factor on how well an application that makes use of it performs. Completeness can be improved by performing knowledge enrichment. Duplicate detection aims to find identity links between the instances of knowledge graphs and is a fundamental subtask of knowledge enrichment. Current solutions to the problem require expert knowledge of the tool and the knowledge graph they are applied to. Users might not have this expert knowledge. We present our service-based approach to the duplicate detection task that provides an easy-to-use no-code solution that is still competitive with the
CTC Alignments Improve Autoregressive Translation,3,Connectionist Temporal Classification (CTC) is a widely used approach for automatic speech recognition (ASR) that performs conditionally independent monotonic alignment. However for translation CTC exhibits clear limitations due to the contextual and non-monotonic nature of the task and thus lags behind attentional decoder approaches in terms of translation quality. In this work we argue that CTC does in fact make sense for translation if applied in a joint CTC/attention framework wherein CTC's core properties can counteract several key weaknesses of pure-attention models during training and decoding. To validate this conjecture we modify the Hybrid CTC/Attention model originally proposed for ASR
Privacy-Preserving Link Prediction,2,Consider two data holders ABC and XYZ with graph data (e.g. social networks e-commerce telecommunication and bio-informatics). ABC can see that node A is linked to node B and XYZ can see node B is linked to node C. Node B is the common neighbour of A and C but neither network can discover this fact on their own. In this paper we provide a two party computation that ABC and XYZ can run to discover the common neighbours in the union of their graph data however neither party has to reveal their plaintext graph to the other. Based on
An efficient heuristic approach combining maximal itemsets and area measure for compressing voluminous table constraints,1,Constraint Programming is a powerful paradigm to model and solve combinatorial problems. While there are many kinds of constraints the table constraint is perhaps the most significant-being the most well-studied and has the ability to encode any other constraints defined on finite variables. However constraints can be very voluminous and their size can grow exponentially with their arity. To reduce space and the time complexity researchers have focused on various forms of compression. In this paper we propose a new approach based on maximal frequent itemsets technique and area measure for enumerating the maximal frequent itemsets relevant for compressing table
SQL and NoSQL Databases Software architectures performance analysis and assessments -- A Systematic Literature review,1,Context: The efficient processing of Big Data is a challenging task for SQL and NoSQL Databases where competent software architecture plays a vital role. The SQL Databases are designed for structuring data and supporting vertical scalability. In contrast horizontal scalability is backed by NoSQL Databases and can process sizeable unstructured Data efficiently. One can choose the right paradigm according to the organisation's needs; however making the correct choice can often be challenging. The SQL and NoSQL Databases follow different architectures. Also the mixed model is followed by each category of NoSQL Databases. Hence data movement becomes difficult for cloud consumers
Contrast Pattern Mining: A Survey,1,Contrast pattern mining (CPM) is an important and popular subfield of data mining. Traditional sequential patterns cannot describe the contrast information between different classes of data while contrast patterns involving the concept of contrast can describe the significant differences between datasets under different contrast conditions. Based on the number of papers published in this field we find that researchers' interest in CPM is still active. Since CPM has many research questions and research methods. It is difficult for new researchers in the field to understand the general situation of the field in a short period of time. Therefore the purpose
"CQE in OWL 2 QL: A ""Longest Honeymoon"" Approach (extended version)",1,Controlled Query Evaluation (CQE) has been recently studied in the context of Semantic Web ontologies. The goal of CQE is concealing some query answers so as to prevent external users from inferring confidential information. In general there exist multiple mutually incomparable ways of concealing answers and previous CQE approaches choose in advance which answers are visible and which are not. In this paper instead we study a dynamic CQE method namely we propose to alter the answer to the current query based on the evaluation of previous ones. We aim at a system that besides being able to protect confidential
Printing variability of copy detection patterns,2,Copy detection pattern (CDP) is a novel solution for products' protection against counterfeiting which gains its popularity in recent years. CDP attracts the anti-counterfeiting industry due to its numerous benefits in comparison to alternative protection techniques. Besides its attractiveness there is an essential gap in the fundamental analysis of CDP authentication performance in large-scale industrial applications. It concerns variability of CDP parameters under different production conditions that include a type of printer substrate printing resolution etc. Since digital off-set printing represents great flexibility in terms of product personalized in comparison with traditional off-set printing it looks very interesting to address
Dimensional Data KNN-Based Imputation,1,Data Warehouses (DWs) are core components of Business Intelligence (BI). Missing data in DWs have a great impact on data analyses. Therefore missing data need to be completed. Unlike other existing data imputation methods mainly adapted for facts we propose a new imputation method for dimensions. This method contains two steps: 1) a hierarchical imputation and 2) a k-nearest neighbors (KNN) based imputation. Our solution has the advantage of taking into account the DW structure and dependency constraints. Experimental assessments validate our method in terms of effectiveness and efficiency.
Hiding Images in Deep Probabilistic Models,2,Data hiding with deep neural networks (DNNs) has experienced impressive successes in recent years. A prevailing scheme is to train an autoencoder consisting of an encoding network to embed (or transform) secret messages in (or into) a carrier and a decoding network to extract the hidden messages. This scheme may suffer from several limitations regarding practicability security and embedding capacity. In this work we describe a different computational framework to hide images in deep probabilistic models. Specifically we use a DNN to model the probability density of cover images and hide a secret image in one particular location of the
Movement Analytics: Current Status Application to Manufacturing and Future Prospects from an AI Perspective,0,Data-driven decision making is becoming an integral part of manufacturing companies. Data is collected and commonly used to improve efficiency and produce high quality items for the customers. IoT-based and other forms of object tracking are an emerging tool for collecting movement data of objects/entities (e.g. human workers moving vehicles trolleys etc.) over space and time. Movement data can provide valuable insights like process bottlenecks resource utilization effective working time etc. that can be used for decision making and improving efficiency. Turning movement data into valuable information for industrial management and decision making requires analysis methods. We refer to this
Seminaive Materialisation in DatalogMTL,1,DatalogMTL is an extension of Datalog with metric temporal operators that has found applications in temporal ontology-based data access and query answering as well as in stream reasoning. Practical algorithms for DatalogMTL are reliant on materialisation-based reasoning where temporal facts are derived in a forward chaining manner in successive rounds of rule applications. Current materialisation-based procedures are however based on a naive evaluation strategy where the main source of inefficiency stems from redundant computations. In this paper we propose a materialisation-based procedure which analogously to the classical seminaive algorithm in Datalog aims at minimising redundant computation by ensuring that each
Preprocessors Matter! Realistic Decision-Based Attacks on Machine Learning Systems,2,Decision-based adversarial attacks construct inputs that fool a machine-learning model into making targeted mispredictions by making only hard-label queries. For the most part these attacks have been applied directly to isolated neural network models. However in practice machine learning models are just a component of a much larger system. By adding just a single preprocessor in front of a classifier we find that state-of-the-art query-based attacks are as much as seven times less effective at attacking a prediction pipeline than attacking the machine learning model alone. Hence attacks that are unaware of this invariance inevitably waste a large number of
Learning-Based Data Storage [Vision] (Technical Report),1,"Deep neural network (DNN) and its variants have been extensively used for a wide spectrum of real applications such as image classification face/speech recognition fraud detection and so on. In addition to many important machine learning tasks as artificial networks emulating the way brain cells function DNNs also show the capability of storing non-linear relationships between input and output data which exhibits the potential of storing data via DNNs. We envision a new paradigm of data storage ""DNN-as-a-Database"" where data are encoded in well-trained machine learning models. Compared with conventional data storage that directly records data in raw formats learning-based"
UnGANable: Defending Against GAN-based Face Manipulation,2,Deepfakes pose severe threats of visual misinformation to our society. One representative deepfake application is face manipulation that modifies a victim's facial attributes in an image e.g. changing her age or hair color. The state-of-the-art face manipulation techniques rely on Generative Adversarial Networks (GANs). In this paper we propose the first defense system namely UnGANable against GAN-inversion-based face manipulation. In specific UnGANable focuses on defending GAN inversion an essential step for face manipulation. Its core technique is to search for alternative images (called cloaked images) around the original images (called target images) in image space. When posted online these cloaked
Privacy-Patterns for IoT Application Developers,2,Designing Internet of things (IoT) applications (apps) is challenging due to the heterogeneous nature of the systems on which these apps are deployed. Personal data often classified as sensitive may be collected and analysed by IoT apps where data privacy laws are expected to protect such information. Various approaches already exist to support privacy-by-design (PbD) schemes enabling developers to take data privacy into account at the design phase of application development. However developers are not widely adopting these approaches because of understandability and interpretation challenges. A limited number of tools currently exist to assist developers in this context -- leading
A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models,3,Despite the remarkable success of pre-trained language models (PLMs) they still face two challenges: First large-scale PLMs are inefficient in terms of memory footprint and computation. Second on the downstream tasks PLMs tend to rely on the dataset bias and struggle to generalize to out-of-distribution (OOD) data. In response to the efficiency problem recent studies show that dense PLMs can be replaced with sparse subnetworks without hurting the performance. Such subnetworks can be found in three scenarios: 1) the fine-tuned PLMs 2) the raw PLMs and then fine-tuned in isolation and even inside 3) PLMs without any parameter fine-tuning. However
NeuDep: Neural Binary Memory Dependence Analysis,2,Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries. We present a new machine-learning-based approach to predict memory dependencies by exploiting the model's learned knowledge about how binary programs execute. Our approach features (i) a self-supervised procedure that pretrains a neural
Social Influence Dialogue Systems: A Scoping Survey of the Efforts Towards Influence Capabilities of Dialogue Systems,3,Dialogue systems capable of social influence such as persuasion negotiation and therapy are essential for extending the use of technology to numerous realistic scenarios. However existing research primarily focuses on either task-oriented or open-domain scenarios a categorization that has been inadequate for capturing influence skills systematically. There exists no formal definition or category for dialogue systems with these skills and data-driven efforts in this direction are highly limited. In this work we formally define and introduce the category of \emph{social influence dialogue systems} that influence users' cognitive and emotional responses leading to changes in thoughts opinions and behaviors through natural
Instance Regularization for Discriminative Language Model Pre-training,3,Discriminative pre-trained language models (PrLMs) can be generalized as denoising auto-encoders that work with two procedures ennoising and denoising. First an ennoising process corrupts texts with arbitrary noising functions to construct training instances. Then a denoising language model is trained to restore the corrupted tokens. Existing studies have made progress by optimizing independent strategies of either ennoising or denosing. They treat training instances equally throughout the training process with little attention on the individual contribution of those instances. To model explicit signals of instance contribution this work proposes to estimate the complexity of restoring the original sentences from corrupted ones
OpenDSU: Digital Sovereignty in PharmaLedger,2,Distributed ledger networks chiefly those based on blockchain technologies currently are heralding a next generation of computer systems that aims to suit modern users' demands. Over the recent years several technologies for blockchains off-chaining strategies as well as decentralised and respectively self-sovereign identity systems have shot up so fast that standardisation of the protocols is lagging behind severely hampering the interoperability of different approaches. Moreover most of the currently available solutions for distributed ledgers focus on either home users or enterprise use case scenarios failing to provide integrative solutions addressing the needs of both. Herein we introduce the OpenDSU platform
Model Cascading: Towards Jointly Improving Efficiency and Accuracy of NLP Systems,3,Do all instances need inference through the big models for a correct prediction? Perhaps not; some instances are easy and can be answered correctly by even small capacity models. This provides opportunities for improving the computational efficiency of systems. In this work we present an explorative study on 'model cascading' a simple technique that utilizes a collection of models of varying capacities to accurately yet efficiently output predictions. Through comprehensive experiments in multiple task settings that differ in the number of models available for cascading (K value) we show that cascading improves both the computational efficiency and the prediction accuracy.
Dominance-based Rough Set Approach basic ideas and main trends,0,Dominance-based Rough Approach (DRSA) has been proposed as a machine learning and knowledge discovery methodology to handle Multiple Criteria Decision Aiding (MCDA). Due to its capacity of asking the decision maker (DM) for simple preference information and supplying easily understandable and explainable recommendations DRSA gained much interest during the years and it is now one of the most appreciated MCDA approaches. In fact it has been applied also beyond MCDA domain as a general knowledge discovery and data mining methodology for the analysis of monotonic (and also non-monotonic) data. In this contribution we recall the basic principles and the main
"ILX: Intelligent ""Location+X"" Data Systems (Vision Paper)",1,Due to the ubiquity of mobile phones and location-detection devices location data is being generated in very large volumes. Queries and operations that are performed on location data warrant the use of database systems. Despite that location data is being supported in data systems as an afterthought. Typically relational or NoSQL data systems that are mostly designed with non-location data in mind get extended with spatial or spatiotemporal indexes some query operators and higher level syntactic sugar in order to support location data. The ubiquity of location data and location data services call for systems that are solely designed and
Decompiling x86 Deep Neural Network Executables,2,Due to their widespread use on heterogeneous hardware devices deep learning (DL) models are compiled into executables by DL compilers to fully leverage low-level hardware primitives. This approach allows DL computations to be undertaken at low cost across a variety of computing platforms including CPUs GPUs and various hardware accelerators. We present BTD (Bin to DNN) a decompiler for deep neural network (DNN) executables. BTD takes DNN executables and outputs full model specifications including types of DNN operators network topology dimensions and parameters that are (nearly) identical to those of the input models. BTD delivers a practical framework to process
Natural Language Processing for Cognitive Analysis of Emotions,3,Emotion analysis in texts suffers from two major limitations: annotated gold-standard corpora are mostly small and homogeneous and emotion identification is often simplified as a sentence-level classification problem. To address these issues we introduce a new annotation scheme for exploring emotions and their causes along with a new French dataset composed of autobiographical accounts of an emotional scene. The texts were collected by applying the Cognitive Analysis of Emotions developed by A. Finkel to help people improve on their emotion management. The method requires the manual analysis of an emotional event by a coach trained in Cognitive Analysis. We present
Comparison of encrypted control approaches and tutorial on dynamic systems using LWE-based homomorphic encryption,2,Encrypted control has been introduced to protect controller data by encryption at the stage of computation and communication by performing the computation directly on encrypted data. In this article we first review and categorize recent relevant studies on encrypted control. Approaches based on homomorphic encryption multi-party computation and secret sharing are introduced compared and then discussed with respect to computational complexity communication load enabled operations security and research directions. We proceed to discuss a current challenge in the application of homomorphic encryption to dynamic systems where arithmetic operations other than integer addition and multiplication are limited. We also introduce a
ClusterEA: Scalable Entity Alignment with Stochastic Training and Normalized Mini-batch Similarities,1,Entity alignment (EA) aims at finding equivalent entities in different knowledge graphs (KGs). Embedding-based approaches have dominated the EA task in recent years. Those methods face problems that come from the geometric properties of embedding vectors including hubness and isolation. To solve these geometric problems many normalization approaches have been adopted for EA. However the increasing scale of KGs renders it hard for EA models to adopt the normalization processes thus limiting their usage in real-world applications. To tackle this challenge we present ClusterEA a general framework that is capable of scaling up EA models and enhancing their results by
Privacy-preserving Decentralized Federated Learning over Time-varying Communication Graph,2,Establishing how a set of learners can provide privacy-preserving federated learning in a fully decentralized (peer-to-peer no coordinator) manner is an open problem. We propose the first privacy-preserving consensus-based algorithm for the distributed learners to achieve decentralized global model aggregation in an environment of high mobility where the communication graph between the learners may vary between successive rounds of model aggregation. In particular in each round of global model aggregation the Metropolis-Hastings method is applied to update the weighted adjacency matrix based on the current communication topology. In addition the Shamir's secret sharing scheme is integrated to facilitate privacy in
Model-Driven Engineering for Formal Verification and Security Testing of Authentication Protocols,2,Even if the verification of authentication protocols can be achieved by means of formal analysis the modelling of such an activity is an error-prone task due to the lack of automated and integrated processes. This paper proposes a comprehensive approach based on the Unified Modeling Language (UML) profiling technique and on model-transformation to enable automatic analysis of authentication protocols starting from high-level models. In particular a UML-based approach is able to generate an annotated model of communication protocols from which formal notations (e.g. AnBx Tamarin) can be generated. Such models in lower-level languages can be analysed with existing solvers and/or
Blockchain-based Monitoring for Poison Attack Detection in Decentralized Federated Learning,2,Federated Learning (FL) is a machine learning technique that addresses the privacy challenges in terms of access rights of local datasets by enabling the training of a model across nodes holding their data samples locally. To achieve decentralized federated learning blockchain-based FL was proposed as a distributed FL architecture. In decentralized FL the chief is eliminated from the learning process as workers collaborate between each other to train the global model. Decentralized FL applications need to account for the additional delay incurred by blockchain-based FL deployments. Particularly in this setting to detect targeted/untargeted poisoning attacks we investigate the end-to-end learning
CSS: Combining Self-training and Self-supervised Learning for Few-shot Dialogue State Tracking,3,Few-shot dialogue state tracking (DST) is a realistic problem that trains the DST model with limited labeled data. Existing few-shot methods mainly transfer knowledge learned from external labeled dialogue data (e.g. from question answering dialogue summarization machine reading comprehension tasks etc.) into DST whereas collecting a large amount of external labeled data is laborious and the external data may not effectively contribute to the DST-specific task. In this paper we propose a few-shot DST framework called CSS which Combines Self-training and Self-supervised learning methods. The unlabeled data of the DST task is incorporated into the self-training iterations where the pseudo
SEE-Few: Seed Expand and Entail for Few-shot Named Entity Recognition,3,Few-shot named entity recognition (NER) aims at identifying named entities based on only few labeled instances. Current few-shot NER methods focus on leveraging existing datasets in the rich-resource domains which might fail in a training-from-scratch setting where no source-domain data is used. To tackle training-from-scratch setting it is crucial to make full use of the annotation information (the boundaries and entity types). Therefore in this paper we propose a novel multi-task (Seed Expand and Entail) learning framework SEE-Few for Few-shot NER without using source domain data. The seeding and expanding modules are responsible for providing as accurate candidate spans as
Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models,3,Fine-tuning large pretrained language models on a limited training corpus usually suffers from poor generalization. Prior works show that the recently-proposed sharpness-aware minimization (SAM) optimization method can improve the model generalization. However SAM adds a perturbation to each model parameter equally (but not all parameters contribute equally to the optimization of training) which we argue is sub-optimal and will lead to excessive computation. In this paper we propose a novel optimization procedure namely FSAM which introduces a Fisher mask to improve the efficiency and performance of SAM. In short instead of adding perturbation to all parameters FSAM uses the Fisher
Towards Target High-Utility Itemsets,1,For applied intelligence utility-driven pattern discovery algorithms can identify insightful and useful patterns in databases. However in these techniques for pattern discovery the number of patterns can be huge and the user is often only interested in a few of those patterns. Hence targeted high-utility itemset mining has emerged as a key research topic where the aim is to find a subset of patterns that meet a targeted pattern constraint instead of all patterns. This is a challenging task because efficiently finding tailored patterns in a very large search space requires a targeted mining algorithm. A first algorithm called TargetUM
Rethinking the Event Coding Pipeline with Prompt Entailment,3,"For monitoring crises political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types a task referred to as event coding. Typically domain experts craft an event type ontology annotators label a large dataset and technical experts develop a supervised coding system. In this work we propose PR-ENT a new event coding approach that is more flexible and resource-efficient while maintaining competitive accuracy: first we extend an event description such as ""Military injured two civilians''"
Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering,3,Given its effectiveness on knowledge-intensive natural language processing tasks dense retrieval models have become increasingly popular. Specifically the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This bi-encoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture Task-aware Specialization for dense Retrieval (TASER) which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five
Integrating connection search in graph queries,1,Graph data management and querying has many practical applications. When graphs are very heterogeneous and/or users are unfamiliar with their structure they may need to find how two or more groups of nodes are connected in a graph even when users are not able to describe the connections. This is only partially supported by existing query languages which allow searching for paths but not for trees connecting three or more node groups. The latter is related to the NP-hard Group Steiner Tree problem and has been previously considered for keyword search in databases. In this work we formally show how
Temporal Fuzzy Utility Maximization with Remaining Measure,1,High utility itemset mining approaches discover hidden patterns from large amounts of temporal data. However an inescapable problem of high utility itemset mining is that its discovered results hide the quantities of patterns which causes poor interpretability. The results only reflect the shopping trends of customers which cannot help decision makers quantify collected information. In linguistic terms computers use mathematical or programming languages that are precisely formalized but the language used by humans is always ambiguous. In this paper we propose a novel one-phase temporal fuzzy utility itemset mining approach called TFUM. It revises temporal fuzzy-lists to maintain less but
HUE: Pretrained Model and Dataset for Understanding Hanja Documents of Ancient Korea,3,Historical records in Korea before the 20th century were primarily written in Hanja an extinct language based on Chinese characters and not understood by modern Korean or Chinese speakers. Historians with expertise in this time period have been analyzing the documents but that process is very difficult and time-consuming and language models would significantly speed up the process. Toward building and evaluating language models for Hanja we release the Hanja Understanding Evaluation dataset consisting of chronological attribution topic classification named entity recognition and summary retrieval tasks. We also present BERT-based models continued training on the two major corpora from the
Medha: Microcoded Hardware Accelerator for computing on Encrypted Data,2,Homomorphic encryption (HE) enables computation on encrypted data and hence it has a great potential in privacy-preserving outsourcing of computations to the cloud. Hardware acceleration of HE is crucial as software implementations are very slow. In this paper we present design methodologies for building a programmable hardware accelerator for speeding up the cloud-side homomorphic evaluations on encrypted data. First we propose a divide-and-conquer technique that enables homomorphic evaluations in a large polynomial ring $R_{Q2N}$ to use a hardware accelerator that has been built for the smaller ring $R_{QN}$. The technique makes it possible to use a single hardware accelerator flexibly
Advice Conformance Verification by Reinforcement Learning agents for Human-in-the-Loop,0,Human-in-the-loop (HiL) reinforcement learning is gaining traction in domains with large action and state spaces and sparse rewards by allowing the agent to take advice from HiL. Beyond advice accommodation a sequential decision-making agent must be able to express the extent to which it was able to utilize the human advice. Subsequently the agent should provide a means for the HiL to inspect parts of advice that it had to reject in favor of the overall environment objective. We introduce the problem of Advice-Conformance Verification which requires reinforcement learning (RL) agents to provide assurances to the human in the loop
Type theory in human-like learning and inference,0,"Humans can generate reasonable answers to novel queries (Schulz 2012): if I asked you what kind of food you want to eat for lunch you would respond with a food not a time. The thought that one would respond ""After 4pm"" to ""What would you like to eat"" is either a joke or a mistake and seriously entertaining it as a lunch option would likely never happen in the first place. While understanding how people come up with new ideas thoughts explanations and hypotheses that obey the basic constraints of a novel search space is of central importance to cognitive"
Towards an efficient and risk aware strategy for guiding farmers in identifying best crop management,0,Identification of best performing fertilizer practices among a set of contrasting practices with field trials is challenging as crop losses are costly for farmers. To identify best management practices an ''intuitive strategy'' would be to set multi-year field trials with equal proportion of each practice to test. Our objective was to provide an identification strategy using a bandit algorithm that was better at minimizing farmers' losses occurring during the identification compared with the ''intuitive strategy''. We used a modification of the Decision Support Systems for Agro-Technological Transfer (DSSAT) crop model to mimic field trial responses with a case-study in Southern
Separate and conquer heuristic allows robust mining of contrast sets from various types of data,1,Identifying differences between groups is one of the most important knowledge discovery problems. The procedure also known as contrast sets mining is applied in a wide range of areas like medicine industry or economics. In the paper we present RuleKit-CS an algorithm for contrast set mining based on a sequential covering - a well established heuristic for decision rule induction. Multiple passes accompanied with an attribute penalization scheme allow generating contrast sets describing same examples with different attributes unlike the standard sequential covering. The ability to identify contrast sets in regression and survival data sets the feature not provided by
Faster Secure Comparisons with Offline Phase for Efficient Private Set Intersection,2,In a Private section intersection (PSI) protocol Alice and Bob compute the intersection of their respective sets without disclosing any element not in the intersection. PSI protocols have been extensively studied in the literature and are deployed in industry. With state-of-the-art protocols achieving optimal asymptotic complexity performance improvements are rare and can only improve complexity constants. In this paper we present a new private extremely efficient comparison protocol that leads to a PSI protocol with low constants. A useful property of our comparison protocol is that it can be divided into an online and an offline phase. All expensive cryptographic
Goal Recognition as a Deep Learning Task: the GRNet Approach,0,In automated planning recognising the goal of an agent from a trace of observations is an important task with many applications. The state-of-the-art approaches to goal recognition rely on the application of planning techniques which requires a model of the domain actions and of the initial domain state (written e.g. in PDDL). We study an alternative approach where goal recognition is formulated as a classification task addressed by machine learning. Our approach called GRNet is primarily aimed at making goal recognition more accurate as well as faster by learning how to solve it in a given domain. Given a planning
Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning,0,In cooperative multi-agent reinforcement learning a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on properties of the environment -- its spatial layout distribution of obstacles dynamics etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and
Relational Message Passing for Fully Inductive Knowledge Graph Completion,0,In knowledge graph completion (KGC) predicting triples involving emerging entities and/or relations which are unseen when the KG embeddings are learned has become a critical challenge. Subgraph reasoning with message passing is a promising and popular solution. Some recent methods have achieved good performance but they (i) usually can only predict triples involving unseen entities alone failing to address more realistic fully inductive situations with both unseen entities and unseen relations and (ii) often conduct message passing over the entities with the relation patterns not fully utilized. In this study we propose a new method named RMPI which uses a
XInsight: eXplainable Data Analysis Through The Lens of Causality,1,In light of the growing popularity of Exploratory Data Analysis (EDA) understanding the underlying causes of the knowledge acquired by EDA is crucial but remains under-researched. This study promotes for the first time a transparent and explicable perspective on data analysis called eXplainable Data Analysis (XDA). XDA provides data analysis with qualitative and quantitative explanations of causal and non-causal semantics. This way XDA will significantly improve human understanding and confidence in the outcomes of data analysis facilitating accurate data interpretation and decision-making in the real world. For this purpose we present XInsight a general framework for XDA. XInsight is a
Continuous Monte Carlo Graph Search,0,In many complex sequential decision making tasks online planning is crucial for high-performance. For efficient online planning Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off between exploration and exploitation. MCTS outperforms comparison methods in various discrete decision making domains such as Go Chess and Shogi. Following extensions of MCTS to continuous domains have been proposed. However the inherent high branching factor and the resulting explosion of search tree size is limiting existing methods. To solve this problem this paper proposes Continuous Monte Carlo Graph Search (CMCGS) a novel extension of MCTS to online planning in environments
Towards Target Sequential Rules,1,In many real-world applications sequential rule mining (SRM) can provide prediction and recommendation functions for a variety of services. It is an important technique of pattern mining to discover all valuable rules that belong to high-frequency and high-confidence sequential rules. Although several algorithms of SRM are proposed to solve various practical problems there are no studies on target sequential rules. Targeted sequential rule mining aims at mining the interesting sequential rules that users focus on thus avoiding the generation of other invalid and unnecessary rules. This approach can further improve the efficiency of users in analyzing rules and reduce the
When one Logic is Not Enough: Integrating First-order Annotations in OWL Ontologies,0,In ontology development there is a gap between domain ontologies which mostly use the web ontology language OWL and foundational ontologies written in first-order logic FOL. To bridge this gap we present Gavel a tool that supports the development of heterogeneous 'FOWL' ontologies that extend OWL with FOL annotations and is able to reason over the combined set of axioms. Since FOL annotations are stored in OWL annotations FOWL ontologies remain compatible with the existing OWL infrastructure. We show that for the OWL domain ontology OBI the stronger integration with its FOL top-level ontology BFO via our approach enables us
pMPL: A Robust Multi-Party Learning Framework with a Privileged Party,2,In order to perform machine learning among multiple parties while protecting the privacy of raw data privacy-preserving machine learning based on secure multi-party computation (MPL for short) has been a hot spot in recent. The configuration of MPL usually follows the peer-to-peer architecture where each party has the same chance to reveal the output result. However typical business scenarios often follow a hierarchical architecture where a powerful usually privileged party leads the tasks of machine learning. Only the privileged party can reveal the final model even if other assistant parties collude with each other. It is even required to avoid
Manipulation and Peer Mechanisms: A Survey,0,In peer mechanisms the competitors for a prize also determine who wins. Each competitor may be asked to rank grade or nominate peers for the prize. Since the prize can be valuable such as financial aid course grades or an award at a conference competitors may be tempted to manipulate the mechanism. We survey approaches to prevent or discourage the manipulation of peer mechanisms. We conclude our survey by identifying several important research challenges
Exploring Adaptive MCTS with TD Learning in miniXCOM,0,In recent years Monte Carlo tree search (MCTS) has achieved widespread adoption within the game community. Its use in conjunction with deep reinforcement learning has produced success stories in many applications. While these approaches have been implemented in various games from simple board games to more complicated video games such as StarCraft the use of deep neural networks requires a substantial training period. In this work we explore on-line adaptivity in MCTS without requiring pre-training. We present MCTS-TD an adaptive MCTS algorithm improved with temporal difference learning. We demonstrate our new approach on the game miniXCOM a simplified version of
Differentially Private Propensity Scores for Bias Correction,2,In surveys it is typically up to the individuals to decide if they want to participate or not which leads to participation bias: the individuals willing to share their data might not be representative of the entire population. Similarly there are cases where one does not have direct access to any data of the target population and has to resort to publicly available proxy data sampled from a different distribution. In this paper we present Differentially Private Propensity Scores for Bias Correction (DiPPS) a method for approximating the true data distribution of interest in both of the above settings. We
On the complexity of finding set repairs for data-graphs,1,In the deeply interconnected world we live in pieces of information link domains all around us. As graph databases embrace effectively relationships among data and allow processing and querying these connections efficiently they are rapidly becoming a popular platform for storage that supports a wide range of domains and applications. As in the relational case it is expected that data preserves a set of integrity constraints that define the semantic structure of the world it represents. When a database does not satisfy its integrity constraints a possible approach is to search for a 'similar' database that does satisfy the constraints
A Survey: Implementations of Non-fungible Token System in Different Fields,2,In the realm of digital art and collectibles NFTs are sweeping the board. Because of the massive sales to a new crypto audience the livelihoods of digital artists are being transformed. It is no surprise that celebs are jumping on the bandwagon. It is a fact that NFTs can be used in multiple ways including digital artwork such as animation character design digital painting collection of selfies or vlogs and many more digital entities. As a result they may be used to signify the possession of any specific object whether it be digital or physical. NFTs are digital tokens that
Current injection and voltage insertion attacks against the VMG-KLJN secure key exchanger,2,In this paper the vulnerability of the Vadai Mingesz and Gingl (VMG)- Kirchhoff-Law-Johnson-Noise (KLJN) Key Exchanger (Nature Science Report 5 (2015) 13653) against two active attacks is demonstrated. The security vulnerability arises from the fact that the effective driving impedances are different between the HL and LH cases for the VMG-KLJN scheme; whereas for the ideal KLJN scheme they are same. Two defense schemes are shown against these attacks but each of them can protect against only one of the attack types; but not against the two attacks simultaneously. The theoretical results are confirmed by computer simulations.
On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding,3,In this paper we examine the use of semantically-aligned speech representations for end-to-end spoken language understanding (SLU). We employ the recently-introduced SAMU-XLSR model which is designed to generate a single embedding that captures the semantics at the utterance level semantically aligned across different languages. This model combines the acoustic frame-level speech representation learning model (XLS-R) with the Language Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the SAMU-XLSR model instead of the initial XLS-R model improves significantly the performance in the framework of end-to-end SLU. Finally we present the benefits of using this model towards language
Estimating the hardness of SAT encodings for Logical Equivalence Checking of Boolean circuits,0,In this paper we investigate how to estimate the hardness of Boolean satisfiability (SAT) encodings for the Logical Equivalence Checking problem (LEC). Meaningful estimates of hardness are important in cases when a conventional SAT solver cannot solve a SAT instance in a reasonable time. We show that the hardness of SAT encodings for LEC instances can be estimated \textit{w.r.t.} some SAT partitioning. We also demonstrate the dependence of the accuracy of the resulting estimates on the probabilistic characteristics of a specially defined random variable associated with the considered partitioning. The paper proposes several methods for constructing partitionings which when used
BanglaParaphrase: A High-Quality Bangla Paraphrase Dataset,3,In this work we present BanglaParaphrase a high-quality synthetic Bangla Paraphrase dataset curated by a novel filtering pipeline. We aim to take a step towards alleviating the low resource status of the Bangla language in the NLP domain through the introduction of BanglaParaphrase which ensures quality by preserving both semantics and diversity making it particularly useful to enhance other Bangla datasets. We show a detailed comparative analysis between our dataset and models trained on it with other existing works to establish the viability of our synthetic paraphrase data generation pipeline. We are making the dataset and models publicly available at
Multi-Agent Chance-Constrained Stochastic Shortest Path with Application to Risk-Aware Intelligent Intersection,0,In transportation networks where traffic lights have traditionally been used for vehicle coordination intersections act as natural bottlenecks. A formidable challenge for existing automated intersections lies in detecting and reasoning about uncertainty from the operating environment and human-driven vehicles. In this paper we propose a risk-aware intelligent intersection system for autonomous vehicles (AVs) as well as human-driven vehicles (HVs). We cast the problem as a novel class of Multi-agent Chance-Constrained Stochastic Shortest Path (MCC-SSP) problems and devise an exact Integer Linear Programming (ILP) formulation that is scalable in the number of agents' interaction points (e.g. potential collision points at the
Query-based Industrial Analytics over Knowledge Graphs with Ontology Reshaping,1,Industrial analytics that includes among others equipment diagnosis and anomaly detection heavily relies on integration of heterogeneous production data. Knowledge Graphs (KGs) as the data format and ontologies as the unified data schemata are a prominent solution that offers high quality data integration and a convenient and standardised way to exchange data and to layer analytical applications over it. However poor design of ontologies of high degree of mismatch between them and industrial data naturally lead to KGs of low quality that impede the adoption and scalability of industrial analytics. Indeed such KGs substantially increase the training time of writing
ASTA: Learning Analytical Semantics over Tables for Intelligent Data Analysis and Visualization,1,Intelligent analysis and visualization of tables use techniques to automatically recommend useful knowledge from data thus freeing users from tedious multi-dimension data mining. While many studies have succeeded in automating recommendations through rules or machine learning it is difficult to generalize expert knowledge and provide explainable recommendations. In this paper we present the recommendation of conditional formatting for the first time together with chart recommendation to exemplify intelligent table analysis. We propose analytical semantics over tables to uncover common analysis pattern behind user-created analyses. Here we design analytical semantics by separating data focus from user intent which extract the user
Network Intrusion Detection System in a Light Bulb,2,Internet of Things (IoT) devices are progressively being utilised in a variety of edge applications to monitor and control home and industry infrastructure. Due to the limited compute and energy resources active security protections are usually minimal in many IoT devices. This has created a critical security challenge that has attracted researchers' attention in the field of network security. Despite a large number of proposed Network Intrusion Detection Systems (NIDSs) there is limited research into practical IoT implementations and to the best of our knowledge no edge-based NIDS has been demonstrated to operate on common low-power chipsets found in the
From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models,3,Investigating better ways to reuse the released pre-trained language models (PLMs) can significantly reduce the computational cost and the potential environmental side-effects. This paper explores a novel PLM reuse paradigm Knowledge Integration (KI). Without human annotations available KI aims to merge the knowledge from different teacher-PLMs each of which specializes in a different classification problem into a versatile student model. To achieve this we first derive the correlation between virtual golden supervision and teacher predictions. We then design a Model Uncertainty--aware Knowledge Integration (MUKI) framework to recover the golden supervision for the student. Specifically MUKI adopts Monte-Carlo Dropout to estimate
Graphical Join: A New Physical Join Algorithm for RDBMSs,1,Join operations (especially n-way many-to-many joins) are known to be time- and resource-consuming. At large scales with respect to table and join-result sizes current state of the art approaches (including both binary-join plans which use Nested-loop/Hash/Sort-merge Join algorithms or alternatively worst-case optimal join algorithms (WOJAs)) may even fail to produce any answer given reasonable resource and time constraints. In this work we introduce a new approach for n-way equi-join processing the Graphical Join (GJ). The key idea is two-fold: First to map the physical join computation problem to PGMs and introduce tweaked inference algorithms which can compute a Run-Length Encoding
PatternRank: Leveraging Pretrained Language Models and Part of Speech for Unsupervised Keyphrase Extraction,3,Keyphrase extraction is the process of automatically selecting a small set of most relevant phrases from a given text. Supervised keyphrase extraction approaches need large amounts of labeled training data and perform poorly outside the domain of the training data (Bennani-Smires et al. 2018). In this paper we present PatternRank which leverages pretrained language models and part-of-speech for unsupervised keyphrase extraction from single documents. Our experiments show PatternRank achieves higher precision recall and F1 -scores than previous state-of-the-art approaches. In addition we present the KeyphraseVectorizers package which allows easy modification of part-of-speech patterns for candidate keyphrase selection and hence adaptation
Knowledge Graph Curation: A Practical Framework,1,Knowledge Graphs (KGs) have shown to be very important for applications such as personal assistants question-answering systems and search engines. Therefore it is crucial to ensure their high quality. However KGs inevitably contain errors duplicates and missing values which may hinder their adoption and utility in business applications as they are not curated e.g. low-quality KGs produce low-quality applications that are built on top of them. In this vision paper we propose a practical knowledge graph curation framework for improving the quality of KGs. First we define a set of quality metrics for assessing the status of KGs Second we
Knowledge Management System with NLP-Assisted Annotations: A Brief Survey and Outlook,1,Knowledge management systems are in high demand for industrial researchers chemical or research enterprises or evidence-based decision making. However existing systems have limitations in categorizing and organizing paper insights or relationships. Traditional databases are usually disjoint with logging systems which limit its utility in generating concise collated overviews. In this work we briefly survey existing approaches of this problem space and propose a unified framework that utilizes relational databases to log hierarchical information to facilitate the research and writing process or generate useful knowledge from references or insights from connected concepts. This framework of knowledge management system enables novel functionalities
WawPart: Workload-Aware Partitioning of Knowledge Graphs,1,Large-scale datasets in the form of knowledge graphs are often used in numerous domains today. A knowledge graphs size often exceeds the capacity of a single computer system especially if the graph must be stored in main memory. To overcome this knowledge graphs can be partitioned into multiple sub-graphs and distributed as shards among many computing nodes. However performance of many common tasks performed on graphs such as querying suffers as a result. This is due to distributed joins mandated by graph edges crossing (cutting) the partitions. In this paper we propose a method of knowledge graph partitioning that takes
AWAPart: Adaptive Workload-Aware Partitioning of Knowledge Graphs,1,Large-scale knowledge graphs are increasingly common in many domains. Their large sizes often exceed the limits of systems storing the graphs in a centralized data store especially if placed in main memory. To overcome this large knowledge graphs need to be partitioned into multiple sub-graphs and placed in nodes in a distributed system. But querying these fragmented sub-graphs poses new challenges such as increased communication costs due to distributed joins involving cut edges. To combat these problems a good partitioning should reduce the edge cuts while considering a given query workload. However a partitioned graph needs to be continually re-partitioned
LSI: A Learned Secondary Index Structure,1,Learned index structures have been shown to achieve favorable lookup performance and space consumption compared to their traditional counterparts such as B-trees. However most learned index studies have focused on the primary indexing setting where the base data is sorted. In this work we investigate whether learned indexes sustain their advantage in the secondary indexing setting. We introduce Learned Secondary Index (LSI) a first attempt to use learned indexes for indexing unsorted data. LSI works by building a learned index over a permutation vector which allows binary search to performed on the unsorted base data using random access. We additionally
Legal Element-oriented Modeling with Multi-view Contrastive Learning for Legal Case Retrieval,3,Legal case retrieval which aims to retrieve relevant cases given a query case plays an essential role in the legal system. While recent research efforts improve the performance of traditional ad-hoc retrieval models legal case retrieval is still challenging since queries are legal cases which contain hundreds of tokens. Legal cases are much longer and more complicated than keywords queries. Apart from that the definition of legal relevance is beyond the general definition. In addition to general topical relevance the relevant cases also involve similar situations and legal elements which can support the judgment of the current case. In this
Entity Disambiguation with Entity Definitions,3,Local models have recently attained astounding performances in Entity Disambiguation (ED) with generative and extractive formulations being the most promising research directions. However previous works limited their studies to using as the textual representation of each candidate only its Wikipedia title. Although certainly effective this strategy presents a few critical issues especially when titles are not sufficiently informative or distinguishable from one another. In this paper we address this limitation and investigate to what extent more expressive textual representations can mitigate it. We thoroughly evaluate our approach against standard benchmarks in ED and find extractive formulations to be particularly well-suited
Capturing Global Structural Information in Long Document Question Answering with Compressive Graph Selector Network,3,Long document question answering is a challenging task due to its demands for complex reasoning over long text. Previous works usually take long documents as non-structured flat texts or only consider the local structure in long documents. However these methods usually ignore the global structure of the long document which is essential for long-range understanding. To tackle this problem we propose Compressive Graph Selector Network (CGSN) to capture the global structure in a compressive and iterative manner. Specifically the proposed model consists of three modules: local graph network global graph network and evidence memory network. Firstly the local graph network
Synthetic Dataset Generation for Privacy-Preserving Machine Learning,2,Machine Learning (ML) has achieved enormous success in solving a variety of problems in computer vision speech recognition object detection to name a few. The principal reason for this success is the availability of huge datasets for training deep neural networks (DNNs). However datasets cannot be publicly released if they contain sensitive information such as medical records and data privacy becomes a major concern. Encryption methods could be a possible solution however their deployment on ML applications seriously impacts classification accuracy and results in substantial computational overhead. Alternatively obfuscation techniques could be used but maintaining a good trade-off between visual
Inferring Tabular Analysis Metadata by Infusing Distribution and Knowledge Information,1,Many data analysis tasks heavily rely on a deep understanding of tables (multi-dimensional data). Across the tasks there exist comonly used metadata attributes of table fields / columns. In this paper we identify four such analysis metadata: Measure/dimension dichotomy common field roles semantic field type and default aggregation function. While those metadata face challenges of insufficient supervision signals utilizing existing knowledge and understanding distribution. To inference these metadata for a raw table we propose our multi-tasking Metadata model which fuses field distribution and knowledge graph information into pre-trained tabular models. For model training and evaluation we collect a large corpus
Mixture of Attention Heads: Selecting Attention Heads Per Token,3,Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA) a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input a router dynamically selects a subset of $k$ attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer.
The Effects of Data Quality on Machine Learning Performance,1,Modern artificial intelligence (AI) applications require large quantities of training and test data. This need creates critical challenges not only concerning the availability of such data but also regarding its quality. For example incomplete erroneous or inappropriate training data can lead to unreliable models that produce ultimately poor decisions. Trustworthy AI applications require high-quality training and test data along many dimensions such as accuracy completeness consistency and uniformity. We explore empirically the relationship between six of the traditional data quality dimensions and the performance of fifteen widely used machine learning (ML) algorithms covering the tasks of classification regression and clustering
An IoT-Enriched Event Log for Process Mining in Smart Factories,1,Modern technologies such as the Internet of Things (IoT) are becoming increasingly important in various domains including Business Process Management (BPM) research. One main research area in BPM is process mining which can be used to analyze event logs e.g. for checking the conformance of running processes. However there are only a few IoT-based event logs available for research purposes. Some of them are artificially generated and the problem occurs that they do not always completely reflect the actual physical properties of smart environments. In this paper we present an IoT-enriched XES event log that is generated by a physical
Cyber-Resilient Privacy Preservation and Secure Billing Approach for Smart Energy Metering Devices,2,Most of the smart applications such as smart energy metering devices demand strong privacy preservation to strengthen data privacy. However it is difficult to protect the privacy of the smart device data especially on the client side. It is mainly because payment for billing is computed by the server deployed at the client's side and it is highly challenging to prevent the leakage of client's information to unauthorised users. Various researchers have discussed this problem and have proposed different privacy preservation techniques. Conventional techniques suffer from the problem of high computational and communication overload on the client side. In addition
Belief functions on ordered frames of discernment,0,Most questionnaires offer ordered responses whose order is poorly studied via belief functions. In this paper we study the consequences of a frame of discernment consisting of ordered elements on belief functions. This leads us to redefine the power space and the union of ordered elements for the disjunctive combination. We also study distances on ordered elements and their use. In particular from a membership function we redefine the cardinality of the intersection of ordered elements considering them fuzzy.
Regret Analysis of the Stochastic Direct Search Method for Blind Resource Allocation,0,Motivated by programmatic advertising optimization we consider the task of sequentially allocating budget across a set of resources. At every time step a feasible allocation is chosen and only a corresponding random return is observed. The goal is to maximize the cumulative expected sum of returns. This is a realistic model for budget allocation across subdivisions of marketing campaigns when the objective is to maximize the number of conversions. We study direct search (aka pattern search) methods for linearly constrained and derivative-free optimization in the presence of noise. Those algorithms are easy to implement and particularly suited to constrained optimization.
MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples,2,Multi-label classification which predicts a set of labels for an input has many applications. However multiple recent studies showed that multi-label classification is vulnerable to adversarial examples. In particular an attacker can manipulate the labels predicted by a multi-label classifier for an input via adding carefully crafted human-imperceptible perturbation to it. Existing provable defenses for multi-class classification achieve sub-optimal provable robustness guarantees when generalized to multi-label classification. In this work we propose MultiGuard the first provably robust defense against adversarial examples to multi-label classification. Our MultiGuard leverages randomized smoothing which is the state-of-the-art technique to build provably robust classifiers. Specifically
Effective Metaheuristic Based Classifiers for Multiclass Intrusion Detection,2,Network security has become the biggest concern in the area of cyber security because of the exponential growth in computer networks and applications. Intrusion detection plays an important role in the security of information systems or networks devices. The purpose of an intrusion detection system (IDS) is to detect malicious activities and then generate an alarm against these activities. Having a large amount of data is one of the key problems in detecting attacks. Most of the intrusion detection systems use all features of datasets to evaluate the models and result in is low detection rate high computational time and
Safety Verification for Neural Networks Based on Set-boundary Analysis,0,Neural networks (NNs) are increasingly applied in safety-critical systems such as autonomous vehicles. However they are fragile and are often ill-behaved. Consequently their behaviors should undergo rigorous guarantees before deployment in practice. In this paper we propose a set-boundary reachability method to investigate the safety verification problem of NNs from a topological perspective. Given an NN with an input set and a safe set the safety verification problem is to determine whether all outputs of the NN resulting from the input set fall within the safe set. In our method the homeomorphism property of NNs is mainly exploited which establishes
Neurosymbolic Programming for Science,0,Neurosymbolic Programming (NP) techniques have the potential to accelerate scientific discovery across fields. These models combine neural and symbolic components to learn complex patterns and representations from data using high-level concepts or known constraints. As a result NP techniques can interface with symbolic domain knowledge from scientists such as prior knowledge and experimental context to produce interpretable outputs. Here we identify opportunities and challenges between current NP models and scientific workflows with real-world examples from behavior analysis in science. We define concrete next steps to move the NP for science field forward to enable its use broadly for workflows across
FastPacket: Towards Pre-trained Packets Embedding based on FastText for next-generation NIDS,2,New Attacks are increasingly used by attackers everyday but many of them are not detected by Intrusion Detection Systems as most IDS ignore raw packet information and only care about some basic statistical information extracted from PCAP files. Using networking programs to extract fixed statistical features from packets is good but may not enough to detect nowadays challenges. We think that it is time to utilize big data and deep learning for automatic dynamic feature extraction from packets. It is time to get inspired by deep learning pre-trained models in computer vision and natural language processing so security deep learning
DIGAT: Modeling News Recommendation with Dual-Graph Interaction,3,News recommendation (NR) is essential for online news services. Existing NR methods typically adopt a news-user representation learning framework facing two potential limitations. First in news encoder single candidate news encoding suffers from an insufficient semantic information problem. Second existing graph-based NR methods are promising but lack effective news-user feature interaction rendering the graph-based recommendation suboptimal. To overcome these limitations we propose dual-interactive graph attention networks (DIGAT) consisting of news- and user-graph channels. In the news-graph channel we enrich the semantics of single candidate news by incorporating the semantically relevant news information with a semantic-augmented graph (SAG). In the user-graph
The Fungibility of Non-Fungible Tokens: A Quantitative Analysis of ERC-721 Metadata,2,Non-Fungible Tokens (NFTs) digital certificates of ownership for virtual art have until recently been traded on a highly lucrative and speculative market. Yet an emergence of misconceptions along with a sustained market downtime are calling the value of NFTs into question. This project (1) describes three properties that any valuable NFT should possess (permanence immutability and uniqueness) (2) creates a quantitative summary of permanence as an initial criteria and (3) tests our measures on 6 months of NFTs on the Ethereum blockchain finding 45% of ERC721 tokens in our corpus do not satisfy this initial criteria. Our work could help
Viterbi Decoding of Directed Acyclic Transformer for Non-Autoregressive Machine Translation,3,Non-autoregressive models achieve significant decoding speedup in neural machine translation but lack the ability to capture sequential dependency. Directed Acyclic Transformer (DA-Transformer) was recently proposed to model sequential dependency with a directed acyclic graph. Consequently it has to apply a sequential decision process at inference time which harms the global translation accuracy. In this paper we present a Viterbi decoding framework for DA-Transformer which guarantees to find the joint optimal solution for the translation and decoding path under any length constraint. Experimental results demonstrate that our approach consistently improves the performance of DA-Transformer while maintaining a similar decoding speedup.
An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification,3,Non-hierarchical sparse attention Transformer-based models such as Longformer and Big Bird are popular approaches to working with long documents. There are clear benefits to these approaches compared to the original Transformer in terms of efficiency but Hierarchical Attention Transformer (HAT) models are a vastly understudied alternative. We develop and release fully pre-trained HAT models that use segment-wise followed by cross-segment encoders and compare them with Longformer models and partially pre-trained HATs. In several long document downstream classification tasks our best HAT model outperforms equally-sized Longformer models while using 10-20% less GPU memory and processing documents 40-45% faster. In a series
A Generic Algorithm for Top-K On-Shelf Utility Mining,1,On-shelf utility mining (OSUM) is an emerging research direction in data mining. It aims to discover itemsets that have high relative utility in their selling time period. Compared with traditional utility mining OSUM can find more practical and meaningful patterns in real-life applications. However there is a major drawback to traditional OSUM. For normal users it is hard to define a minimum threshold minutil for mining the right amount of on-shelf high utility itemsets. On one hand if the threshold is set too high the number of patterns would not be enough. On the other hand if the threshold is
Enabling a Zero Trust Architecture in a 5G-enabled Smart Grid,2,One of the most promising applications of the IoT is the Smart Grid (SG). Integrating SG's data communications network into the power grid allows gathering and analyzing information from power lines distribution power stations and end users. A smart grid (SG) requires a prompt and dependable connection to provide real-time monitoring through the IoT. Hence 5G could be considered a catalyst for upgrading the existing power grid systems. Nonetheless the additional attack surface of information infrastructure has been brought about by the widespread adoption of ubiquitous connectivity in 5G to which the typical information security system in the smart grid
Generalized Optimality Guarantees for Solving Continuous Observation POMDPs through Particle Belief MDP Approximation,0,Partially observable Markov decision processes (POMDPs) provide a flexible representation for real-world decision and control problems. However POMDPs are notoriously difficult to solve especially when the state and observation spaces are continuous or hybrid which is often the case for physical systems. While recent online sampling-based POMDP algorithms that plan with observation likelihood weighting have shown practical effectiveness a general theory bounding the approximation error of the particle filtering techniques that these algorithms use has not previously been proposed. Our main contribution is to formally justify that optimality guarantees in a finite sample particle belief MDP (PB-MDP) approximation of a
Citation Trajectory Prediction via Publication Influence Representation Using Temporal Knowledge Graph,0,Predicting the impact of publications in science and technology has become an important research area which is useful in various real world scenarios such as technology investment research direction selection and technology policymaking. Citation trajectory prediction is one of the most popular tasks in this area. Existing approaches mainly rely on mining temporal and graph data from academic articles. Some recent methods are capable of handling cold-start prediction by aggregating metadata features of new publications. However the implicit factors causing citations and the richer information from handling temporal and attribute features still need to be explored. In this paper we
Opportunistic Qualitative Planning in Stochastic Systems with Incomplete Preferences over Reachability Objectives,0,Preferences play a key role in determining what goals/constraints to satisfy when not all constraints can be satisfied simultaneously. In this paper we study how to synthesize preference satisfying plans in stochastic systems modeled as an MDP given a (possibly incomplete) combinative preference model over temporally extended goals. We start by introducing new semantics to interpret preferences over infinite plays of the stochastic system. Then we introduce a new notion of improvement to enable comparison between two prefixes of an infinite play. Based on this we define two solution concepts called safe and positively improving (SPI) and safe and almost-surely
Are Pretrained Multilingual Models Equally Fair Across Languages?,3,Pretrained multilingual language models can help bridge the digital language divide enabling high-quality NLP models for lower resourced languages. Studies of multilingual models have so far focused on performance consistency and cross-lingual generalisation. However with their wide-spread application in the wild and downstream societal impact it is important to put multilingual models under the same scrutiny as monolingual models. This work investigates the group fairness of multilingual models asking whether these models are equally fair across languages. To this end we create a new four-way multilingual dataset of parallel cloze test examples (MozArt) equipped with demographic information (balanced with regard
GANTouch: An Attack-Resilient Framework for Touch-based Continuous Authentication System,2,Previous studies have shown that commonly studied (vanilla) implementations of touch-based continuous authentication systems (V-TCAS) are susceptible to active adversarial attempts. This study presents a novel Generative Adversarial Network assisted TCAS (G-TCAS) framework and compares it to the V-TCAS under three active adversarial environments viz. Zero-effort Population and Random-vector. The Zero-effort environment was implemented in two variations viz. Zero-effort (same-dataset) and Zero-effort (cross-dataset). The first involved a Zero-effort attack from the same dataset while the second used three different datasets. G-TCAS showed more resilience than V-TCAS under the Population and Random-vector the more damaging adversarial scenarios than the Zero-effort. On
Towards Structure-aware Paraphrase Identification with Phrase Alignment Using Sentence Encoders,3,Previous works have demonstrated the effectiveness of utilising pre-trained sentence encoders based on their sentence representations for meaning comparison tasks. Though such representations are shown to capture hidden syntax structures the direct similarity comparison between them exhibits weak sensitivity to word order and structural differences in given sentences. A single similarity score further makes the comparison process hard to interpret. Therefore we here propose to combine sentence encoders with an alignment component by representing each sentence as a list of predicate-argument spans (where their span representations are derived from sentence encoders) and decomposing the sentence-level meaning comparison into the alignment
Time-aware topic identification in social media with pre-trained language models: A case study of electric vehicles,3,Recent extensively competitive business environment makes companies to keep their eyes on social media as there is a growing recognition over customer languages (e.g. needs interests and complaints) as source of future opportunities. This research avenue analysing social media data has received much attention in academia but their utilities are limited as most of methods provide retrospective results. Moreover the increasing number of customer-generated contents and rapidly varying topics have made the necessity of time-aware topic evolution analyses. Recently several researchers have showed the applicability of pre-trained semantic language models to social media as an input feature but leaving limitations
Bad Citrus: Reducing Adversarial Costs with Model Distances,2,Recent work by Jia et al. showed the possibility of effectively computing pairwise model distances in weight space using a model explanation technique known as LIME. This method requires query-only access to the two models under examination. We argue this insight can be leveraged by an adversary to reduce the net cost (number of queries) of launching an evasion campaign against a deployed model. We show that there is a strong negative correlation between the success rate of adversarial transfer and the distance between the victim model and the surrogate used to generate the evasive samples. Thus we propose and
Continual Training of Language Models for Few-Shot Learning,3,Recent work on applying large language models (LMs) achieves impressive performance in many NLP applications. Adapting or posttraining an LM using an unlabeled domain corpus can produce even better performance for end-tasks in the domain. This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills. The goal is to improve the few-shot end-task learning in these domains. The resulting system is called CPT (Continual PostTraining) which to our knowledge is the first continual post-training system. Experimental results verify its
Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training,3,Recently knowledge-enhanced pre-trained language models (KEPLMs) improve context-aware representations via learning from structured relations in knowledge graphs and/or linguistic knowledge from syntactic or dependency analysis. Unlike English there is a lack of high-performing open-source Chinese KEPLMs in the natural language processing (NLP) community to support various language understanding applications. In this paper we revisit and advance the development of Chinese natural language understanding with a series of novel Chinese KEPLMs released in various parameter sizes namely CKBERT (Chinese knowledge-enhanced BERT).Specifically both relational and linguistic knowledge is effectively injected into CKBERT based on two novel pre-training tasks i.e. linguistic-aware masked language
Experiential Explanations for Reinforcement Learning,0,Reinforcement Learning (RL) approaches are becoming increasingly popular in various key disciplines including robotics and healthcare. However many of these systems are complex and non-interpretable making it challenging for non-AI experts to understand or intervene. One of the challenges of explaining RL agent behavior is that when learning to predict future expected reward agents discard contextual information about their experiences when training in an environment and rely solely on expected utility. We propose a technique Experiential Explanations for generating local counterfactual explanations that can answer users' why-not questions by explaining qualitatively the effects of the various environmental rewards on the
A Human Rights-Based Approach to Responsible AI,0,Research on fairness accountability transparency and ethics of AI-based interventions in society has gained much-needed momentum in recent years. However it lacks an explicit alignment with a set of normative values and principles that guide this research and interventions. Rather an implicit consensus is often assumed to hold for the values we impart into our models - something that is at odds with the pluralistic world we live in. In this paper we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework
Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA,3,Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA) which provides more comprehensive information. However training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity problem. To address the above challenges we introduce an optimized OpenQA Table-Text Retriever (OTTeR) to jointly retrieve tabular and textual evidences. Firstly we propose to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed-modality negative sampling strategy. Secondly to alleviate data sparsity problem and enhance the general retrieval ability we conduct retrieval-centric mixed-modality synthetic pre-training. Experimental results demonstrate that OTTeR
StreamingHub: Interactive Stream Analysis Workflows,1,Reusable data/code and reproducible analyses are foundational to quality research. This aspect however is often overlooked when designing interactive stream analysis workflows for time-series data (e.g. eye-tracking data). A mechanism to transmit informative metadata alongside data may allow such workflows to intelligently consume data propagate metadata to downstream tasks and thereby auto-generate reusable reproducible analytic outputs with zero supervision. Moreover a visual programming interface to design develop and execute such workflows may allow rapid prototyping for interdisciplinary research. Capitalizing on these ideas we propose StreamingHub a framework to build metadata propagating interactive stream analysis workflows using visual programming. We conduct
Reinforcement Learning Approach for Multi-Agent Flexible Scheduling Problems,0,Scheduling plays an important role in automated production. Its impact can be found in various fields such as the manufacturing industry the service industry and the technology industry. A scheduling problem (NP-hard) is a task of finding a sequence of job assignments on a given set of machines with the goal of optimizing the objective defined. Methods such as Operation Research Dispatching Rules and Combinatorial Optimization have been applied to scheduling problems but no solution guarantees to find the optimal solution. The recent development of Reinforcement Learning has shown success in sequential decision-making problems. This research presents a Reinforcement Learning
Enriching Vulnerability Reports Through Automated and Augmented Description Summarization,2,Security incidents and data breaches are increasing rapidly and only a fraction of them is being reported. Public vulnerability databases e.g. national vulnerability database (NVD) and common vulnerability and exposure (CVE) have been leading the effort in documenting vulnerabilities and sharing them to aid defenses. Both are known for many issues including brief vulnerability descriptions. Those descriptions play an important role in communicating the vulnerability information to security analysts in order to develop the appropriate countermeasure. Many resources provide additional information about vulnerabilities however they are not utilized to boost public repositories. In this paper we devise a pipeline to
KnowGraph-PM: a Knowledge Graph based Pricing Model for Semiconductors Supply Chains,1,Semiconductor supply chains are described by significant demand fluctuation that increases as one moves up the supply chain the so-called bullwhip effect. To counteract semiconductor manufacturers aim to optimize capacity utilization to deliver with shorter lead times and exploit this to generate revenue. Additionally in a competitive market firms seek to maintain customer relationships while applying revenue management strategies such as dynamic pricing. Price change potentially generates conflicts with customers. In this paper we present KnowGraph-PM a knowledge graph-based dynamic pricing model. The semantic model uses the potential of faster delivery and shorter lead times to define premium prices thus
How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,3,Several multi-hop reading comprehension datasets have been proposed to resolve the issue of reasoning shortcuts by which questions can be answered without performing multi-hop reasoning. However the ability of multi-hop models to perform step-by-step reasoning when finding an answer to a comparison question remains unclear. It is also unclear how questions about the internal reasoning process are useful for training and evaluating question-answering (QA) systems. To evaluate the model precisely in a hierarchical manner we first propose a dataset \textit{HieraDate} with three probing tasks in addition to the main question: extraction reasoning and robustness. Our dataset is created by enhancing
Dataset Distillation for Medical Dataset Sharing,2,Sharing medical datasets between hospitals is challenging because of the privacy-protection problem and the massive cost of transmitting and storing many high-resolution medical images. However dataset distillation can synthesize a small dataset such that models trained on it achieve comparable performance with the original large dataset which shows potential for solving the existing medical sharing problems. Hence this paper proposes a novel dataset distillation-based method for medical dataset sharing. Experimental results on a COVID-19 chest X-ray image dataset show that our method can achieve high detection performance even using scarce anonymized chest X-ray images.
A Secure Federated Learning Framework for Residential Short Term Load Forecasting,2,Smart meter measurements though critical for accurate demand forecasting face several drawbacks including consumers' privacy data breach issues to name a few. Recent literature has explored Federated Learning (FL) as a promising privacy-preserving machine learning alternative which enables collaborative learning of a model without exposing private raw data for short term load forecasting. Despite its virtue standard FL is still vulnerable to an intractable cyber threat known as Byzantine attack carried out by faulty and/or malicious clients. Therefore to improve the robustness of federated short-term load forecasting against Byzantine threats we develop a state-of-the-art differentially private secured FL-based framework that
Red-Teaming the Stable Diffusion Safety Filter,0,Stable Diffusion is a recent open-source image generation model comparable to proprietary models such as DALLE Imagen or Parti. Stable Diffusion comes with a safety filter that aims to prevent generating explicit images. Unfortunately the filter is obfuscated and poorly documented. This makes it hard for users to prevent misuse in their applications and to understand the filter's limitations and improve it. We first show that it is easy to generate disturbing content that bypasses the safety filter. We then reverse-engineer the filter and find that while it aims to prevent sexual content it ignores violence gore and other similarly
CHAE: Fine-Grained Controllable Story Generation with Characters Actions and Emotions,3,Story generation has emerged as an interesting yet challenging NLP task in recent years. Some existing studies aim at generating fluent and coherent stories from keywords and outlines; while others attempt to control the global features of the story such as emotion style and topic. However these works focus on coarse-grained control on the story neglecting control on the details of the story which is also crucial for the task. To fill the gap this paper proposes a model for fine-grained control on the story which allows the generation of customized stories with characters corresponding actions and emotions arbitrarily assigned.
Mind's Eye: Grounded Language Model Reasoning through Simulation,3,Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes and then use the simulation results as part of the input
Graph Neural Network Policies and Imitation Learning for Multi-Domain Task-Oriented Dialogues,3,Task-oriented dialogue systems are designed to achieve specific goals while conversing with humans. In practice they may have to handle simultaneously several domains and tasks. The dialogue manager must therefore be able to take into account domain changes and plan over different domains/tasks in order to deal with multidomain dialogues. However learning with reinforcement in such context becomes difficult because the state-action dimension is larger while the reward signal remains scarce. Our experimental results suggest that structured policies based on graph neural networks combined with different degrees of imitation learning can effectively handle multi-domain dialogues. The reported experiments underline the
Finding and Exploring Promising Search Space for the 0-1 Multidimensional Knapsack Problem,0,The 0-1 multidimensional knapsack problem(MKP) is a classical NP-hard combinatorial optimization problem. In this paper we propose a novel heuristic algorithm simulating evolutionary computation and large neighbourhood search for the MKP. It maintains a set of solutions and abstracts information from the solution set to generate good partial assignments. To find high-quality solutions integer programming is employed to explore the promising search space specified by the good partial assignments. Extensive experimentation with commonly used benchmark sets shows that our approach outperforms the state of the art heuristic algorithms TPTEA and DQPSO in solution quality. It finds new lower bound for
Atari-5: Distilling the Arcade Learning Environment down to Five Games,0,The Arcade Learning Environment (ALE) has become an essential benchmark for assessing the performance of reinforcement learning algorithms. However the computational cost of generating results on the entire 57-game dataset limits ALE's use and makes the reproducibility of many results infeasible. We propose a novel solution to this problem in the form of a principled methodology for selecting small but representative subsets of environments within a benchmark suite. We applied our method to identify a subset of five ALE games called Atari-5 which produces 57-game median score estimates within 10% of their true values. Extending the subset to 10-games recovers
A Review of Multilingualism in and for Ontologies,0,The Multilingual Semantic Web has been in focus for over a decade. Multilingualism in Linked Data and RDF has shown substantial adoption but this is unclear for ontologies since the last review 15 years ago. One of the design goals for OWL was internationalisation with the aim that an ontology is usable across languages and cultures. Much research to improve on multilingual ontologies has taken place in the meantime and presumably multilingual linked data could use multilingual ontologies. Therefore this review seeks to (i) elucidate and compare the modelling options for multilingual ontologies (ii) examine extant ontologies for their multilingualism
Study and security analysis of the Spanish identity card,2,The National Identity Document is a fundamental piece of documentation for the identification of citizens throughout the world. That is precisely the case of the DNI (Documento Nacional de Identidad) of Spain. Its importance has been enhanced in recent years with the addition of a chip for the authentication of users within telematic administrative services. Thus the document has since been called: electronic DNI or simply DNIe. Sensitive user information is stored in that integrated circuit such as personal and biometric data along with signature and authentication certificates. Some of the functionalities of the DNIe in its current version at
IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces,3,"The ability to extract high-quality translation dictionaries from monolingual word embedding spaces depends critically on the geometric similarity of the spaces -- their degree of ""isomorphism."" We address the root-cause of faulty cross-lingual mapping: that word embedding training resulted in the underlying spaces being non-isomorphic. We incorporate global measures of isomorphism directly into the skipgram loss function successfully increasing the relative isomorphism of trained word embedding spaces and improving their ability to be mapped to a shared cross-lingual space. The result is improved bilingual lexicon induction in general data conditions under domain mismatch and with training algorithm dissimilarities. We release"
LGTBIDS: Layer-wise Graph Theory Based Intrusion Detection System in Beyond 5G,2,The advancement in wireless communication technologies is becoming more demanding and pervasive. One of the fundamental parameters that limit the efficiency of the network are the security challenges. The communication network is vulnerable to security attacks such as spoofing attacks and signal strength attacks. Intrusion detection signifies a central approach to ensuring the security of the communication network. In this paper an Intrusion Detection System based on the framework of graph theory is proposed. A Layerwise Graph Theory-Based Intrusion Detection System (LGTBIDS) algorithm is designed to detect the attacked node. The algorithm performs the layer-wise analysis to extract the vulnerable
Demystifying Quantum Blockchain for Healthcare,2,The application of blockchain technology can be beneficial in the field of healthcare as well as in the fight against the COVID-19 epidemic. In this work the importance of blockchain is analyzed and it is observed that blockchain technology and the processes associated with it will be utilised in the healthcare systems of the future for data acquisition from sensors automatic patient monitoring and secure data storage. This technology substantially simplifies the process of carrying out operations because it can store a substantial quantity of data in a dispersed and secure manner as well as enable access whenever and wherever
Non-Uniformly Terminating Chase: Size and Complexity,1,The chase procedure originally introduced for checking implication of database constraints and later on used for computing data exchange solutions has recently become a central algorithmic tool in rule-based ontological reasoning. In this context a key problem is non-uniform chase termination: does the chase of a database w.r.t. a rule-based ontology terminate? And if this is the case what is the size of the result of the chase? We focus on guarded tuple-generating dependencies (TGDs) which form a robust rule-based ontology language and study the above central questions for the semi-oblivious version of the chase. One of our main findings
The Small Solution Hypothesis for MAPF on Directed Graphs Is True,0,The determination of the computational complexity of multi-agent pathfinding on directed graphs has been an open problem for many years. Only recently it has been established that the problem is NP-hard. Further it has been proved that it is in NP provided the short solution hypothesis for strongly connected digraphs holds. In this paper it is shown that this hypothesis is indeed true.
Process Modeling and Conformance Checking in Healthcare: A COVID-19 Case Study,1,The discipline of process mining has a solid track record of successful applications to the healthcare domain. Within such research space we conducted a case study related to the Intensive Care Unit (ICU) ward of the Uniklinik Aachen hospital in Germany. The aim of this work is twofold: developing a normative model representing the clinical guidelines for the treatment of COVID-19 patients and analyzing the adherence of the observed behavior (recorded in the information system of the hospital) to such guidelines. We show that through conformance checking techniques it is possible to analyze the care process for COVID-19 patients highlighting
Defining Cases and Variants for Object-Centric Event Data,1,The execution of processes leaves traces of event data in information systems. These event data can be analyzed through process mining techniques. For traditional process mining techniques one has to associate each event with exactly one object e.g. the company's customer. Events related to one object form an event sequence called a case. A case describes an end-to-end run through a process. The cases contained in event data can be used to discover a process model detect frequent bottlenecks or learn predictive models. However events encountered in real-life information systems e.g. ERP systems can often be associated with multiple objects.
Explanations as Programs in Probabilistic Logic Programming,0,The generation of comprehensible explanations is an essential feature of modern artificial intelligence systems. In this work we consider probabilistic logic programming an extension of logic programming which can be useful to model domains with relational structure and uncertainty. Essentially a program specifies a probability distribution over possible worlds (i.e. sets of facts). The notion of explanation is typically associated with that of a world so that one often looks for the most probable world as well as for the worlds where the query is true. Unfortunately such explanations exhibit no causal structure. In particular the chain of inferences required
SemTUI: a Framework for the Interactive Semantic Enrichment of Tabular Data,1,The large availability of datasets fosters the use of \acrshort{ml} and \acrshort{ai} technologies to gather insights study trends and predict unseen behaviours out of the world of data. Today gathering and integrating data from different sources is mainly a manual activity that requires the knowledge of expert users at an high cost in terms of both time and money. It is therefore necessary to make the process of gathering and linking data from many different sources affordable to make datasets ready to perform the desired analysis. In this work we propose the development of a comprehensive framework named SemTUI to
Drowsiness detection in drivers with a smartwatch,2,The main objective of this work is to detect early if a driver shows symptoms of sleepiness that indicate that he/she is falling asleep and in that case generate an alert to wake him/her up. To solve this problem an application has been designed that collects various parameters through a smartwatch while driving. First the application detects the driving action. Then it collects information about the most significant physiological variables of a person while driving. On the other hand given the high level of sensitivity of the data managed in the designed application in this work special attention has been
Bicoptor: Two-round Secure Three-party Non-linear Computation without Preprocessing for Privacy-preserving Machine Learning,2,The overhead of non-linear functions dominates the performance of the secure multiparty computation (MPC) based privacy-preserving machine learning (PPML). This work introduces two sets of novel secure three-party computation (3PC) protocols using additive and replicated secret sharing schemes respectively. We name the whole family of protocols as Bicoptor its basis is a new sign determination protocol which relies on a clever use of the truncation protocol proposed in SecureML (S&P 2017). Our 3PC sign determination protocol only requires two communication rounds and does not involve any preprocessing. Such sign determination protocol is well-suited for computing non-linear functions in PPML e.g.
Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,3,The prevalence and perniciousness of fake news have been a critical issue on the Internet which stimulates the development of automatic fake news detection in turn. In this paper we focus on evidence-based fake news detection where several evidences are utilized to probe the veracity of news (i.e. a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on attention mechanisms. Despite their effectiveness they still suffer from three weaknesses. Firstly sequential models fail to integrate the relevant information that is scattered far apart in evidences. Secondly they underestimate
Probabilities of Causation: Adequate Size of Experimental and Observational Samples,0,The probabilities of causation are commonly used to solve decision-making problems. Tian and Pearl derived sharp bounds for the probability of necessity and sufficiency (PNS) the probability of sufficiency (PS) and the probability of necessity (PN) using experimental and observational data. The assumption is that one is in possession of a large enough sample to permit an accurate estimation of the experimental and observational distributions. In this study we present a method for determining the sample size needed for such estimation when a given confidence interval (CI) is specified. We further show by simulation that the proposed sample size delivered
Belief propagation generalizes backpropagation,0,The two most important algorithms in artificial intelligence are backpropagation and belief propagation. In spite of their importance the connection between them is poorly characterized. We show that when an input to backpropagation is converted into an input to belief propagation so that (loopy) belief propagation can be run on it then the result of belief propagation encodes the result of backpropagation; thus backpropagation is recovered as a special case of belief propagation. In other words we prove for apparently the first time that belief propagation generalizes backpropagation. Our analysis is a theoretical contribution which we motivate with the expectation
Unit Selection: Case Study and Comparison with A/B Test Heuristic,0,The unit selection problem defined by Li and Pearl identifies individuals who have desired counterfactual behavior patterns for example individuals who would respond positively if encouraged and would not otherwise. Li and Pearl showed by example that their unit selection model is beyond the A/B test heuristics. In this paper we reveal the essence of the A/B test heuristics which are exceptional cases of the benefit function defined by Li and Pearl. Furthermore We provided more simulated use cases of Li-Pearl's unit selection model to help decision-makers apply their model correctly explaining that A/B test heuristics are generally problematic.
eDWaaS: A Scalable Educational Data Warehouse as a Service,1,The university management is perpetually in the process of innovating policies to improve the quality of service. Intellectual growth of the students the popularity of university are some of the major areas that management strives to improve upon. Relevant historical data is needed in support of taking any decision. Furthermore providing data to various university ranking frameworks is a frequent activity in recent years. The format of such requirement changes frequently which requires efficient manual effort. Maintaining a data warehouse can be a solution to this problem. However both in-house and outsourced implementation of a dedicated data warehouse may not
Broad-persistent Advice for Interactive Reinforcement Learning Scenarios,0,The use of interactive advice in reinforcement learning scenarios allows for speeding up the learning process for autonomous agents. Current interactive reinforcement learning research has been limited to real-time interactions that offer relevant user advice to the current state only. Moreover the information provided by each interaction is not retained and instead discarded by the agent after a single use. In this paper we present a method for retaining and reusing provided knowledge allowing trainers to give general advice relevant to more than just the current state. Results obtained show that the use of broad-persistent advice substantially improves the performance
Federated Boosted Decision Trees with Differential Privacy,2,There is great demand for scalable secure and efficient privacy-preserving machine learning models that can be trained over distributed data. While deep learning models typically achieve the best results in a centralized non-secure setting different models can excel when privacy and communication constraints are imposed. Instead tree-based approaches such as XGBoost have attracted much attention for their high performance and ease of use; in particular they often achieve state-of-the-art results on tabular data. Consequently several recent works have focused on translating Gradient Boosted Decision Tree (GBDT) models like XGBoost into federated settings via cryptographic mechanisms such as Homomorphic Encryption (HE)
On the non-efficient PAC learnability of acyclic conjunctive queries,1,"This note serves three purposes: (i) we provide a self-contained exposition of the fact that conjunctive queries are not efficiently learnable in the Probably-Approximately-Correct (PAC) model paying clear attention to the complicating fact that this concept class lacks the polynomial-size fitting property a property that is tacitly assumed in much of the computational learning theory literature; (ii) we establish a strong negative PAC learnability result that applies to many restricted classes of conjunctive queries (CQs) including acyclic CQs for a wide range of notions of ""acyclicity""; (iii) we show that CQs are efficiently PAC learnable with membership queries."
An Ontological Approach to Analysing Social Service Provisioning,1,This paper introduces ontological concepts required to evaluate and manage the coverage of social services in a Smart City context. Here we focus on the perspective of key stakeholders namely social purpose organizations and the clients they serve. The Compass ontology presented here extends the Common Impact Data Standard by introducing new concepts related to key dimensions: the who (Stakeholder) the what (Need Need Satisfier Outcome) the how (Service Event) and the contributions (tracking resources). The paper first introduces key stakeholders services outcomes events needs and need satisfiers along with their definitions. Second a subset of competency questions are presented
Machine Translation between Spoken Languages and Signed Languages Represented in SignWriting,3,This paper presents work on novel machine translation (MT) systems between spoken and signed languages where signed languages are represented in SignWriting a sign language writing system. Our work seeks to address the lack of out-of-the-box support for signed languages in current MT systems and is based on the SignBank dataset which contains pairs of spoken language text and SignWriting content. We introduce novel methods to parse factorize decode and evaluate SignWriting leveraging ideas from neural factored MT. In a bilingual setup--translating from American Sign Language to (American) English--our method achieves over 30 BLEU while in two multilingual setups--translating in
Estimating productivity gains in digital automation,0,"This paper proposes a novel productivity estimation model to evaluate the effects of adopting Artificial Intelligence (AI) components in a production chain. Our model provides evidence to address the ""AI's"" Solow's Paradox. We provide (i) theoretical and empirical evidence to explain Solow's dichotomy; (ii) a data-driven model to estimate and asses productivity variations; (iii) a methodology underpinned on process mining datasets to determine the business process BP and productivity; (iv) a set of computer simulation parameters; (v) and empirical analysis on labour-distribution. These provide data on why we consider AI Solow's paradox a consequence of metric mismeasurement."
A forensic analysis of the Google Home: repairing compressed data without error correction,2,This paper provides a detailed explanation of the steps taken to extract and repair a Google Home's internal data. Starting with reverse engineering the hardware of a commercial off-the-shelf Google Home internal data is then extracted by desoldering and dumping the flash memory. As error correction is performed by the CPU using an undisclosed method a new alternative method is shown to repair a corrupted SquashFS filesystem under the assumption of a single or double bitflip per gzip-compressed fragment. Finally a new method to handle multiple possible repairs using three-valued logic is presented.
Performances of Symmetric Loss for Private Data from Exponential Mechanism,2,This study explores the robustness of learning by symmetric loss on private data. Specifically we leverage exponential mechanism (EM) on private labels. First we theoretically re-discussed properties of EM when it is used for private learning with symmetric loss. Then we propose numerical guidance of privacy budgets corresponding to different data scales and utility guarantees. Further we conducted experiments on the CIFAR-10 dataset to present the traits of symmetric loss. Since EM is a more generic differential privacy (DP) technique it being robust has the potential for it to be generalized and to make other DP techniques more robust.
Cognitive Models as Simulators: The Case of Moral Decision-Making,0,To achieve desirable performance current AI systems often require huge amounts of training data. This is especially problematic in domains where collecting data is both expensive and time-consuming e.g. where AI systems require having numerous interactions with humans collecting feedback from them. In this work we substantiate the idea of $\textit{cognitive models as simulators}$ which is to have AI systems interact with and collect feedback from cognitive models instead of humans thereby making their training process both less costly and faster. Here we leverage this idea in the context of moral decision-making by having reinforcement learning (RL) agents learn about
Comparing modern techniques for querying data starting from top-k and skyline queries,1,To make intelligent decisions over complex data by discovering a set of interesting options is something that has become very important for users of modern applications. Consequently researchers are studying new techniques to overcome limitations of traditional ways of querying data from databases as top-k queries and skyline queries. Over the past few years new methods have been developed as Flexible Skylines Regret Minimization and Skyline ordering/ranking. The aim of this survey is to describe these techniques and some their possible variants comparing them and explaining how they improve traditional methods.
Security and Privacy Concerns in Cloud-based Scientific and Business Workflows: A Systematic Review,2,Today the number of data-intensive and compute-intensive applications like business and scientific workflows has dramatically increased which made cloud computing more popular in the matter of delivering a large amount of computing resources on demand. On the other hand security is a critical issue affecting the wide adoption of cloud technologies especially for workflows that are mostly dealing with sensitive data and tasks. In this paper we carry out a review of the state-of-the-art on how security and privacy concerns in scientific and business workflows in cloud environments are being addressed and identify the limitations and gaps in the current
Better Than Whitespace: Information Retrieval for Languages without Custom Tokenizers,3,"Tokenization is a crucial step in information retrieval especially for lexical matching algorithms where the quality of indexable tokens directly impacts the effectiveness of a retrieval system. Since different languages have unique properties the design of the tokenization algorithm is usually language-specific and requires at least some lingustic knowledge. However only a handful of the 7000+ languages on the planet benefit from specialized custom-built tokenization algorithms while the other languages are stuck with a ""default"" whitespace tokenizer which cannot capture the intricacies of different languages. To address this challenge we propose a different approach to tokenization for lexical matching retrieval"
Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling,3,Transformer-based models have achieved great success on sentence pair modeling tasks such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs leading to prohibitive computational costs. Recent studies propose dual-encoder and late interaction architectures for faster computation. However the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end this paper introduces a novel paradigm MixEncoder for efficient sentence pair modeling. MixEncoder involves a light-weight cross-attention mechanism. It conducts query encoding only once while modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate
xDBTagger: Explainable Natural Language Interface to Databases Using Keyword Mappings and Schema Graph,1,Translating natural language queries (NLQ) into structured query language (SQL) in interfaces to relational databases is a challenging task that has been widely studied by researchers from both the database and natural language processing communities. Numerous works have been proposed to attack the natural language interfaces to databases (NLIDB) problem either as a conventional pipeline-based or an end-to-end deep-learning-based solution. Nevertheless regardless of the approach preferred such solutions exhibit black-box nature which makes it difficult for potential users targeted by these systems to comprehend the decisions made to produce the translated SQL. To this end we propose xDBTagger an explainable
Like a bilingual baby: The advantage of visually grounding a bilingual language model,3,Unlike most neural language models humans learn language in a rich multi-sensory and often multi-lingual environment. Current language models typically fail to fully capture the complexities of multilingual language use. We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES. We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity. However we find no significant advantage of visual grounding for abstract words. Our results provide additional evidence of the advantages of visually grounded language models and point to the need for more naturalistic
seL4 Microkernel for virtualization use-cases: Potential directions towards a standard VMM,2,Virtualization plays an essential role in providing security to computational systems by isolating execution environments. Many software solutions called hypervisors have been proposed to provide virtualization capabilities. However only a few were designed for being deployed at the edge of the network in devices with fewer computation resources when compared with servers in the Cloud. Among the few lightweight software that can play the hypervisor role seL4 stands out by providing a small Trusted Computing Base and formally verified components enhancing its security. Despite today being more than a decade with seL4 microkernel technology its existing userland and tools are
T5 for Hate Speech Augmented Data and Ensemble,3,We conduct relatively extensive investigations of automatic hate speech (HS) detection using different state-of-the-art (SoTA) baselines over 11 subtasks of 6 different datasets. Our motivation is to determine which of the recent SoTA models is best for automatic hate speech detection and what advantage methods like data augmentation and ensemble may have on the best model if any. We carry out 6 cross-task investigations. We achieve new SoTA on two subtasks - macro F1 scores of 91.73% and 53.21% for subtasks A and B of the HASOC 2020 dataset where previous SoTA are 51.52% and 26.52% respectively. We achieve near-SoTA
Human-AI Coordination via Human-Regularized Search and Learning,0,We consider the problem of making AI agents that collaborate well with humans in partially observable fully cooperative environments given datasets of human behavior. Inspired by piKL a human-data-regularized search method that improves upon a behavioral cloning policy without diverging far away from it we develop a three-step algorithm that achieve strong performance in coordinating with real humans in the Hanabi benchmark. We first use a regularized search algorithm and behavioral cloning to produce a better human model that captures diverse skill levels. Then we integrate the policy regularization idea into reinforcement learning to train a human-like best response to
MTet: Multi-domain Translation for English and Vietnamese,3,We introduce MTet the largest publicly available parallel corpus for English-Vietnamese translation. MTet consists of 4.2M high-quality training sentence pairs and a multi-domain test set refined by the Vietnamese research community. Combining with previous works on English-Vietnamese translation we grow the existing parallel dataset to 6.2M sentence pairs. We also release the first pretrained model EnViT5 for English and Vietnamese languages. Combining both resources our model significantly outperforms previous state-of-the-art results by up to 2 points in translation BLEU score while being 1.6 times smaller.
Saga: A Platform for Continuous Construction and Serving of Knowledge At Scale,1,We introduce Saga a next-generation knowledge construction and serving platform for powering knowledge-based applications at industrial scale. Saga follows a hybrid batch-incremental design to continuously integrate billions of facts about real-world entities and construct a central knowledge graph that supports multiple production use cases with diverse requirements around data freshness accuracy and availability. In this paper we discuss the unique challenges associated with knowledge graph construction at industrial scale and review the main components of Saga and how they address these challenges. Finally we share lessons-learned from a wide array of production use cases powered by Saga.
Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents,3,We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents as positional layout and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new document categories could potentially emerge. We focus exclusively on the zero-shot setting where inference is done on new unseen classes. To address this task we propose a matching-based approach that relies on a pairwise contrastive objective for both pretraining and fine-tuning. Our results show
Developing a Ranking Problem Library (RPLIB) from a data-oriented perspective,1,We present an improved library for the ranking problem called RPLIB. RPLIB includes the following data and features. (1) Real and artificial datasets of both pairwise data (i.e. information about the ranking of pairs of items) and feature data (i.e. a vector of features about each item to be ranked). These datasets range in size (e.g. from small $n=10$ item datasets to large datasets with hundred of items) application (e.g. from sports to economic data) and source (e.g. real versus artificially generated to have particular structures). (2) RPLIB contains code for the most common ranking algorithms such as the linear
Using Argumentation Schemes to Model Legal Reasoning,0,We present argumentation schemes to model reasoning with legal cases. We provide schemes for each of the three stages that take place after the facts are established: factor ascription issue resolution and outcome determination. The schemes are illustrated with examples from a specific legal domain US Trade Secrets law and the wider applicability of these schemes is discussed.
Automated Security Analysis of Exposure Notification Systems,2,We present the first formal analysis and comparison of the security of the two most widely deployed exposure notification systems ROBERT and the Google and Apple Exposure Notification (GAEN) framework. ROBERT is the most popular instalment of the centralised approach to exposure notification in which the risk score is computed by a central server. GAEN in contrast follows the decentralised approach where the user's phone calculates the risk. The relative merits of centralised and decentralised systems have proven to be a controversial question. The majority of the previous analyses have focused on the privacy implications of these systems ours is
Threat Repair with Optimization Modulo Theories,2,We propose a model-based procedure for automatically preventing security threats using formal models. We encode system models and potential threats as satisfiability modulo theory (SMT) formulas. This model allows us to ask security questions as satisfiability queries. We formulate threat prevention as an optimization problem over the same formulas. The outcome of our threat prevention procedure is a suggestion of model attribute repair that eliminates threats. Whenever threat prevention fails we automatically explain why the threat happens. We implement our approach using the state-of-the-art Z3 SMT solver and interface it with the threat analysis tool THREATGET. We demonstrate the value
Establishing Meta-Decision-Making for AI: An Ontology of Relevance Representation and Reasoning,0,We propose an ontology of building decision-making systems with the aim of establishing Meta-Decision-Making for Artificial Intelligence (AI) improving autonomy and creating a framework to build metrics and benchmarks upon. To this end we propose the three parts of Relevance Representation and Reasoning and discuss their value in ensuring safety and mitigating risk in the context of third wave cognitive systems. Our nomenclature reflects the literature on decision-making and our ontology allows researchers that adopt it to frame their work in relation to one or more of these parts.
Swift Markov Logic for Probabilistic Reasoning on Knowledge Graphs,0,We provide a framework for probabilistic reasoning in Vadalog-based Knowledge Graphs (KGs) satisfying the requirements of ontological reasoning: full recursion powerful existential quantification expression of inductive definitions. Vadalog is a Knowledge Representation and Reasoning (KRR) language based on Warded Datalog+/- a logical core language of existential rules with a good balance between computational complexity and expressive power. Handling uncertainty is essential for reasoning with KGs. Yet Vadalog and Warded Datalog+/- are not covered by the existing probabilistic logic programming and statistical relational learning approaches for several reasons including insufficient support for recursion with existential quantification and the impossibility to express
Ontology-Mediated Querying on Databases of Bounded Cliquewidth,1,We study the evaluation of ontology-mediated queries (OMQs) on databases of bounded cliquewidth from the viewpoint of parameterized complexity theory. As the ontology language we consider the description logics $\mathcal{ALC}$ and $\mathcal{ALCI}$ as well as the guarded two-variable fragment GF$_2$ of first-order logic. Queries are atomic queries (AQs) conjunctive queries (CQs) and unions of CQs. All studied OMQ problems are fixed-parameter linear (FPL) when the parameter is the size of the OMQ plus the cliquewidth. Our main contribution is a detailed analysis of the dependence of the running time on the parameter exhibiting several interesting effects.
Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models,3,While multilingual language models can improve NLP performance on low-resource languages by leveraging higher-resource languages they also reduce average performance on all languages (the 'curse of multilinguality'). Here we show another problem with multilingual models: grammatical structures in higher-resource languages bleed into lower-resource languages a phenomenon we call grammatical structure bias. We show this bias via a novel method for comparing the fluency of multilingual models to the fluency of monolingual Spanish and Greek models: testing their preference for two carefully-chosen variable grammatical structures (optional pronoun-drop in Spanish and optional Subject-Verb ordering in Greek). We find that multilingual BERT is
Toward a Generic Mapping Language for Transformations between RDF and Data Interchange Formats,1,While there exist approaches to integrate heterogeneous data using semantic models such semantic models can typically not be used by existing software tools. Many software tools - especially in engineering - only have options to import and export data in more established data interchange formats such as XML or JSON. Thus if an information which is included in a semantic model needs to be used in a such a software tool automatic approaches for mapping semantic information into an interchange format are needed. We aim to develop a generic mapping approach that allows users to create transformations of semantic information
Forecasting SQL Query Cost at Twitter,1,With the advent of the Big Data era it is usually computationally expensive to calculate the resource usages of a SQL query with traditional DBMS approaches. Can we estimate the cost of each query more efficiently without any computation in a SQL engine kernel? Can machine learning techniques help to estimate SQL query resource utilization? The answers are yes. We propose a SQL query cost predictor service which employs machine learning techniques to train models from historical query request logs and rapidly forecasts the CPU and memory resource usages of online queries without any computation in a SQL engine. At
Big data analysis and distributed deep learning for next-generation intrusion detection system optimization,2,With the growing use of information technology in all life domains hacking has become more negatively effective than ever before. Also with developing technologies attacks numbers are growing exponentially every few months and become more sophisticated so that traditional IDS becomes inefficient detecting them. This paper proposes a solution to detect not only new threats with higher detection rate and lower false positive than already used IDS but also it could detect collective and contextual security attacks. We achieve those results by using Networking Chatbot a deep recurrent neural network: Long Short Term Memory (LSTM) on top of Apache Spark
Word Sense Induction with Hierarchical Clustering and Mutual Information Maximization,3,Word sense induction (WSI) is a difficult problem in natural language processing that involves the unsupervised automatic detection of a word's senses (i.e. meanings). Recent work achieves significant results on the WSI task by pre-training a language model that can exclusively disambiguate word senses whereas others employ previously pre-trained language models in conjunction with additional strategies to induce senses. In this paper we propose a novel unsupervised method based on hierarchical clustering and invariant information clustering (IIC). The IIC is used to train a small model to optimize the mutual information between two vector representations of a target word occurring
